\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {chapter}{Table of Contents}{vii}{chapter*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Figures}{viii}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{List of Tables}{xiii}{section*.8}\protected@file@percent }
\citation{Tinto1993}
\citation{Tinto1993}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Project Overview}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Motivation}{1}{section.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces \textbf  {Distribution of Student Outcomes in Dataset.} Pie chart showing the composition of 4,424 students: Graduate (49.9\%, n=2,209), Dropout (32.1\%, n=1,421), Enrolled (17.9\%, n=794). The dataset exhibits moderate class imbalance addressed through stratified sampling and weighted loss functions during model training.}}{2}{figure.1.1}\protected@file@percent }
\newlabel{fig:dataset_distribution}{{1.1}{2}{\textbf {Distribution of Student Outcomes in Dataset.} Pie chart showing the composition of 4,424 students: Graduate (49.9\%, n=2,209), Dropout (32.1\%, n=1,421), Enrolled (17.9\%, n=794). The dataset exhibits moderate class imbalance addressed through stratified sampling and weighted loss functions during model training}{figure.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Objectives}{2}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Methodology}{4}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Project Outcome}{6}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Organization of the Report}{7}{section.1.6}\protected@file@percent }
\citation{Tinto1993}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background and Literature Review}{8}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Preliminaries}{8}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Theoretical Frameworks for Student Retention}{8}{subsection.2.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Tinto's Student Integration Model}{8}{section*.11}\protected@file@percent }
\citation{Bean1985}
\@writefile{toc}{\contentsline {subsubsection}{Bean's Student Attrition Model}{9}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Deep Learning Fundamentals}{10}{subsection.2.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Feedforward Neural Networks}{10}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Attention Mechanisms}{10}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Multi-Task Learning}{10}{section*.15}\protected@file@percent }
\citation{Vaswani2017}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Literature Review}{11}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Educational Data Mining for Student Success}{11}{subsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Attention Mechanisms in Educational Prediction}{11}{subsection.2.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Multi-Task Learning for Educational Outcomes}{12}{subsection.2.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Large Language Models for Educational Recommendations}{12}{subsection.2.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Comparative Analysis with Recent Literature}{12}{subsection.2.2.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Comparison with Recent Literature}}{12}{table.2.1}\protected@file@percent }
\newlabel{tab:literature_gap}{{2.1}{12}{Comparison with Recent Literature}{table.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Gap Analysis}{13}{section.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Summary}{14}{section.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Project Design and Methodology}{15}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Dataset Description and Characteristics}{15}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Dataset Overview}{15}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Feature Categories}{16}{subsection.3.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Feature Categories and Counts}}{16}{table.3.1}\protected@file@percent }
\newlabel{tab:feature_categories}{{3.1}{16}{Feature Categories and Counts}{table.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Complete Feature Listings}{16}{subsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Academic Features (18 features)}{16}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Financial Features (12 features)}{17}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Demographic Features (16 features)}{17}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Feature Ranking and Importance Analysis}{18}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Feature Ranking Across Methods}{18}{subsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Dropout-Specific Feature Importance}{18}{subsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Feature Engineering Strategy}{18}{section.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces \textbf  {Feature Ranking Heatmap.} Comparison of five feature ranking methods (Information Gain, Gini Importance, Gain Ratio, etc.) for top 20 features, showing consensus among methods.}}{19}{figure.3.1}\protected@file@percent }
\newlabel{fig:ranking_heatmap}{{3.1}{19}{\textbf {Feature Ranking Heatmap.} Comparison of five feature ranking methods (Information Gain, Gini Importance, Gain Ratio, etc.) for top 20 features, showing consensus among methods}{figure.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Academic Performance Indicators}{19}{subsection.3.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces \textbf  {Top 20 Features by Information Gain.} Information gain ranking identifies curricular units approved (both semesters) and tuition fees as top predictors of student outcomes.}}{20}{figure.3.2}\protected@file@percent }
\newlabel{fig:ig_ranking}{{3.2}{20}{\textbf {Top 20 Features by Information Gain.} Information gain ranking identifies curricular units approved (both semesters) and tuition fees as top predictors of student outcomes}{figure.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Engagement Metrics}{20}{subsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Socioeconomic Composite Indicators}{20}{subsection.3.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces \textbf  {Top 20 Features by Gini Importance.} Gini-based ranking demonstrates consistency with information gain, validating top features.}}{21}{figure.3.3}\protected@file@percent }
\newlabel{fig:gini_ranking}{{3.3}{21}{\textbf {Top 20 Features by Gini Importance.} Gini-based ranking demonstrates consistency with information gain, validating top features}{figure.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Data Preprocessing Pipeline}{21}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Categorical Encoding}{21}{subsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Feature Normalization}{21}{subsection.3.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces \textbf  {Top 20 Features for Dropout Prediction.} Composite importance score from four feature importance methods, identifying features most predictive of dropout risk.}}{22}{figure.3.4}\protected@file@percent }
\newlabel{fig:dropout_top20}{{3.4}{22}{\textbf {Top 20 Features for Dropout Prediction.} Composite importance score from four feature importance methods, identifying features most predictive of dropout risk}{figure.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Feature Selection}{22}{subsection.3.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Data Partitioning}{22}{subsection.3.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces \textbf  {Comparison of Feature Importance Methods.} Four methods (Tree-based, Permutation, Correlation, Domain Knowledge) applied to dropout prediction, showing method consensus.}}{23}{figure.3.5}\protected@file@percent }
\newlabel{fig:dropout_methods}{{3.5}{23}{\textbf {Comparison of Feature Importance Methods.} Four methods (Tree-based, Permutation, Correlation, Domain Knowledge) applied to dropout prediction, showing method consensus}{figure.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Deep Learning Architectures}{23}{section.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Model 1: Performance Prediction Network (PPN)}{23}{subsection.3.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces \textbf  {Top 20 Features by Gini Importance.} Bar chart ranking features by Random Forest Gini importance. Semester grades dominate (curricular\_units\_*\_sem\_grade), validating Tinto's academic integration theory. Feature selection retained 46 features explaining $\geq $95\% cumulative importance.}}{24}{figure.3.6}\protected@file@percent }
\newlabel{fig:gini_importance}{{3.6}{24}{\textbf {Top 20 Features by Gini Importance.} Bar chart ranking features by Random Forest Gini importance. Semester grades dominate (curricular\_units\_*\_sem\_grade), validating Tinto's academic integration theory. Feature selection retained 46 features explaining $\geq $95\% cumulative importance}{figure.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Model 2: Dropout Prediction Network with Attention (DPN-A)}{24}{subsection.3.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces \textbf  {Top 20 Features by Information Gain.} Entropy-based feature importance ranking. Complementary to Gini importance, demonstrates robust consensus on critical features. Academic variables (success\_rate, average\_grade) and financial indicators (tuition\_fees\_up\_to\_date) emerge as dominant predictors.}}{25}{figure.3.7}\protected@file@percent }
\newlabel{fig:ig_importance}{{3.7}{25}{\textbf {Top 20 Features by Information Gain.} Entropy-based feature importance ranking. Complementary to Gini importance, demonstrates robust consensus on critical features. Academic variables (success\_rate, average\_grade) and financial indicators (tuition\_fees\_up\_to\_date) emerge as dominant predictors}{figure.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Model 3: Hybrid Multi-Task Learning Network (HMTL)}{26}{subsection.3.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Baseline Models for Comparison}{26}{subsection.3.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Large Language Model Integration}{26}{section.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}GPT-4 Recommendation Architecture}{26}{subsection.3.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Evaluation Metrics and Statistical Testing}{27}{section.3.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Multi-Class Metrics (PPN, HMTL Performance Task)}{27}{subsection.3.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Binary Classification Metrics (DPN-A, HMTL Dropout Task)}{27}{subsection.3.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.3}Statistical Significance Testing}{28}{subsection.3.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Summary}{28}{section.3.8}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Implementation and Experimental Setup}{29}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Software Stack and Development Environment}{29}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Programming Language and Libraries}{29}{subsection.4.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Software and Library Specifications}}{29}{table.4.1}\protected@file@percent }
\newlabel{tab:software}{{4.1}{29}{Software and Library Specifications}{table.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Hardware Configuration}{29}{subsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Model Training Procedure}{29}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Training Pipeline and Hyperparameter Tuning}{29}{subsection.4.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Hardware Specifications and Requirements}}{30}{table.4.2}\protected@file@percent }
\newlabel{tab:hardware}{{4.2}{30}{Hardware Specifications and Requirements}{table.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Optimal Hyperparameter Configurations}{30}{subsection.4.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces \textbf  {PPN Hyperparameter Tuning Heatmap.} Accuracy variation across learning rates and batch sizes. Optimal configuration (LR=0.001, BS=32) achieves 77.8\% validation accuracy. Heatmap reveals learning rate 0.001 robustly outperforms alternatives across different batch sizes.}}{31}{figure.4.1}\protected@file@percent }
\newlabel{fig:ppn_heatmap}{{4.1}{31}{\textbf {PPN Hyperparameter Tuning Heatmap.} Accuracy variation across learning rates and batch sizes. Optimal configuration (LR=0.001, BS=32) achieves 77.8\% validation accuracy. Heatmap reveals learning rate 0.001 robustly outperforms alternatives across different batch sizes}{figure.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Training Algorithm}{31}{subsection.4.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Cross-Validation Protocol}{31}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}10-Fold Stratified Cross-Validation}{31}{subsection.4.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces \textbf  {DPN-A Hyperparameter Tuning Heatmap.} Validation accuracy across learning rates and batch sizes. Optimal configuration (LR=0.001, BS=32) achieves 87.05\% accuracy. Demonstrates superior performance and robustness of attention-based architecture.}}{32}{figure.4.2}\protected@file@percent }
\newlabel{fig:dpna_heatmap}{{4.2}{32}{\textbf {DPN-A Hyperparameter Tuning Heatmap.} Validation accuracy across learning rates and batch sizes. Optimal configuration (LR=0.001, BS=32) achieves 87.05\% accuracy. Demonstrates superior performance and robustness of attention-based architecture}{figure.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Reproducibility Provisions}{32}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Random Seed Fixation}{32}{subsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Documentation and Code Availability}{32}{subsection.4.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces \textbf  {HMTL Hyperparameter Tuning Heatmap.} Validation accuracy for multi-task learning configuration. Shows task weighting influence on performance. DPN-A outperforms HMTL, indicating single-task specialization is optimal for this dataset.}}{33}{figure.4.3}\protected@file@percent }
\newlabel{fig:hmtl_heatmap}{{4.3}{33}{\textbf {HMTL Hyperparameter Tuning Heatmap.} Validation accuracy for multi-task learning configuration. Shows task weighting influence on performance. DPN-A outperforms HMTL, indicating single-task specialization is optimal for this dataset}{figure.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Environment Reproducibility}{33}{subsection.4.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Computational Performance}{33}{section.4.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Training Time and Resource Usage}}{33}{table.4.3}\protected@file@percent }
\newlabel{tab:compute_perf}{{4.3}{33}{Training Time and Resource Usage}{table.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Summary}{34}{section.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Experimental Results and Discussion}{35}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Baseline Model Performance}{35}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Random Forest Classifier}{35}{subsection.5.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Random Forest Performance (3-Class Prediction)}}{35}{table.5.1}\protected@file@percent }
\newlabel{tab:rf_perf}{{5.1}{35}{Random Forest Performance (3-Class Prediction)}{table.5.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces \textbf  {Comprehensive Model Performance Comparison.} Bar chart comparing multiple baseline and proposed models across accuracy, F1-score, and other metrics. Shows Random Forest baseline achieving 79.2\% accuracy, with deep learning models achieving competitive or superior performance.}}{36}{figure.5.1}\protected@file@percent }
\newlabel{fig:model_comparison}{{5.1}{36}{\textbf {Comprehensive Model Performance Comparison.} Bar chart comparing multiple baseline and proposed models across accuracy, F1-score, and other metrics. Shows Random Forest baseline achieving 79.2\% accuracy, with deep learning models achieving competitive or superior performance}{figure.5.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Logistic Regression Performance (Binary Dropout)}}{36}{table.5.2}\protected@file@percent }
\newlabel{tab:lr_perf}{{5.2}{36}{Logistic Regression Performance (Binary Dropout)}{table.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Logistic Regression (Dropout Prediction)}{36}{subsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Deep Learning Model Performance}{36}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Performance Prediction Network (PPN)}{36}{subsection.5.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces \textbf  {Target Class Distribution in Educational Dataset.} Pie chart showing the distribution of student outcomes: Graduate (49.9\%), Dropout (32.1\%), Enrolled (17.9\%). Moderate class imbalance addressed through stratified sampling and weighted loss functions.}}{37}{figure.5.2}\protected@file@percent }
\newlabel{fig:class_dist}{{5.2}{37}{\textbf {Target Class Distribution in Educational Dataset.} Pie chart showing the distribution of student outcomes: Graduate (49.9\%), Dropout (32.1\%), Enrolled (17.9\%). Moderate class imbalance addressed through stratified sampling and weighted loss functions}{figure.5.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces PPN Test Set Performance}}{37}{table.5.3}\protected@file@percent }
\newlabel{tab:ppn_perf}{{5.3}{37}{PPN Test Set Performance}{table.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Dropout Prediction Network with Attention (DPN-A)}{37}{subsection.5.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces DPN-A Test Set Performance}}{38}{table.5.4}\protected@file@percent }
\newlabel{tab:dpna_perf}{{5.4}{38}{DPN-A Test Set Performance}{table.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Hybrid Multi-Task Learning Network (HMTL)}{38}{subsection.5.2.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces HMTL Multi-Task Performance}}{38}{table.5.5}\protected@file@percent }
\newlabel{tab:hmtl_perf}{{5.5}{38}{HMTL Multi-Task Performance}{table.5.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Statistical Significance Testing}{38}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}McNemar's Test Results}{38}{subsection.5.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces \textbf  {Confusion Matrix for DPN-A (Attention-Based Dropout Prediction).} Binary classification results showing 94.0\% true negative rate (correctly identified not-at-risk students) and 72.3\% true positive rate (correctly identified at-risk students). The model demonstrates strong specificity suitable for early warning systems.}}{39}{figure.5.3}\protected@file@percent }
\newlabel{fig:dpna_confusion}{{5.3}{39}{\textbf {Confusion Matrix for DPN-A (Attention-Based Dropout Prediction).} Binary classification results showing 94.0\% true negative rate (correctly identified not-at-risk students) and 72.3\% true positive rate (correctly identified at-risk students). The model demonstrates strong specificity suitable for early warning systems}{figure.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Friedman Test for Multiple Models}{39}{subsection.5.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces \textbf  {Confusion Matrices Across All Models.} Comparative visualization showing confusion matrices for Random Forest (baseline), Logistic Regression (baseline), PPN, DPN-A, and HMTL. DPN-A demonstrates the most balanced and accurate predictions across both classes.}}{40}{figure.5.4}\protected@file@percent }
\newlabel{fig:all_confusion}{{5.4}{40}{\textbf {Confusion Matrices Across All Models.} Comparative visualization showing confusion matrices for Random Forest (baseline), Logistic Regression (baseline), PPN, DPN-A, and HMTL. DPN-A demonstrates the most balanced and accurate predictions across both classes}{figure.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Attention Mechanism Analysis}{40}{section.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Feature Importance from Attention Weights}{40}{subsection.5.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces Top 10 Features by Attention Weight}}{40}{table.5.6}\protected@file@percent }
\newlabel{tab:attention_weights}{{5.6}{40}{Top 10 Features by Attention Weight}{table.5.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces \textbf  {ROC Curves for All Models.} Receiver operating characteristic curves showing area under curve (AUC) for each model. DPN-A achieves 0.910 AUC-ROC, demonstrating excellent discrimination ability between at-risk and not-at-risk students.}}{41}{figure.5.5}\protected@file@percent }
\newlabel{fig:roc_curves}{{5.5}{41}{\textbf {ROC Curves for All Models.} Receiver operating characteristic curves showing area under curve (AUC) for each model. DPN-A achieves 0.910 AUC-ROC, demonstrating excellent discrimination ability between at-risk and not-at-risk students}{figure.5.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}SHAP Feature Importance Analysis}{41}{subsection.5.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Cross-Validation Stability}{41}{section.5.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.7}{\ignorespaces 10-Fold Cross-Validation Results}}{41}{table.5.7}\protected@file@percent }
\newlabel{tab:cv_stability}{{5.7}{41}{10-Fold Cross-Validation Results}{table.5.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces \textbf  {Attention Weight Distribution Across Features.} Bar chart showing normalized attention weights for top 15 features in DPN-A. Semester grades (Tinto academic integration factors) dominate with 68.2\% cumulative importance, validating theoretical framework. Tuition fees and scholarship holder (Bean environmental factors) contribute 31.8\%, demonstrating complementary role of environmental factors.}}{42}{figure.5.6}\protected@file@percent }
\newlabel{fig:attention_weights}{{5.6}{42}{\textbf {Attention Weight Distribution Across Features.} Bar chart showing normalized attention weights for top 15 features in DPN-A. Semester grades (Tinto academic integration factors) dominate with 68.2\% cumulative importance, validating theoretical framework. Tuition fees and scholarship holder (Bean environmental factors) contribute 31.8\%, demonstrating complementary role of environmental factors}{figure.5.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}LLM-Generated Recommendations Validation}{42}{section.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}Expert Review Results}{42}{subsection.5.6.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.8}{\ignorespaces GPT-4 Recommendation Quality Metrics}}{42}{table.5.8}\protected@file@percent }
\newlabel{tab:llm_quality}{{5.8}{42}{GPT-4 Recommendation Quality Metrics}{table.5.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Intervention Categories Generated}{42}{subsection.5.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces \textbf  {SHAP Importance: Random Forest Baseline.} Summary plot showing mean absolute SHAP values for Random Forest features. Establishes baseline for comparison with deep learning models.}}{43}{figure.5.7}\protected@file@percent }
\newlabel{fig:shap_rf}{{5.7}{43}{\textbf {SHAP Importance: Random Forest Baseline.} Summary plot showing mean absolute SHAP values for Random Forest features. Establishes baseline for comparison with deep learning models}{figure.5.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Discussion}{43}{section.5.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.1}Key Findings and Interpretations}{43}{subsection.5.7.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces \textbf  {SHAP Importance: Neural Network (DPN-A).} Summary plot showing SHAP values for neural network features. Demonstrates alignment of learned feature importance with attention weights and validates model interpretability.}}{44}{figure.5.8}\protected@file@percent }
\newlabel{fig:shap_nn}{{5.8}{44}{\textbf {SHAP Importance: Neural Network (DPN-A).} Summary plot showing SHAP values for neural network features. Demonstrates alignment of learned feature importance with attention weights and validates model interpretability}{figure.5.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces \textbf  {Cross-Validation Performance Stability.} Boxplots showing accuracy distribution across 10 folds for PPN and DPN-A. Low variance demonstrates robust generalization and consistent performance across different data splits. DPN-A mean: 86.2\% ± 1.8\%.}}{45}{figure.5.9}\protected@file@percent }
\newlabel{fig:cv_stability}{{5.9}{45}{\textbf {Cross-Validation Performance Stability.} Boxplots showing accuracy distribution across 10 folds for PPN and DPN-A. Low variance demonstrates robust generalization and consistent performance across different data splits. DPN-A mean: 86.2\% ± 1.8\%}{figure.5.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.2}Comparison with Literature}{45}{subsection.5.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Summary}{45}{section.5.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces \textbf  {Training Dynamics of DPN-A.} Plot showing training loss, validation loss, and attention weight evolution across 29 epochs. Demonstrates smooth convergence, early stopping at epoch 18 (best validation), and absence of overfitting. Attention mechanism stabilizes after epoch 10.}}{46}{figure.5.10}\protected@file@percent }
\newlabel{fig:training_dynamics}{{5.10}{46}{\textbf {Training Dynamics of DPN-A.} Plot showing training loss, validation loss, and attention weight evolution across 29 epochs. Demonstrates smooth convergence, early stopping at epoch 18 (best validation), and absence of overfitting. Attention mechanism stabilizes after epoch 10}{figure.5.10}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Comprehensive Model Analysis and Comparison}{47}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Feature Selection Optimization Across Models}{47}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Single Classifiers: Decision Tree and Naive Bayes}{47}{subsection.6.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces \textbf  {Single Classifier Hyperparameter Tuning.} Accuracy heatmap for Decision Tree and Naive Bayes across all feature selection methods and feature counts, identifying optimal configurations.}}{48}{figure.6.1}\protected@file@percent }
\newlabel{fig:dt_nb_heatmap}{{6.1}{48}{\textbf {Single Classifier Hyperparameter Tuning.} Accuracy heatmap for Decision Tree and Naive Bayes across all feature selection methods and feature counts, identifying optimal configurations}{figure.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Ensemble Methods: Random Forest, AdaBoost, XGBoost}{48}{subsection.6.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces \textbf  {Comprehensive Metrics: Single Classifiers.} Comparison of Accuracy, Precision, Recall, and F1-Score for Decision Tree and Naive Bayes.}}{49}{figure.6.2}\protected@file@percent }
\newlabel{fig:dt_nb_metrics}{{6.2}{49}{\textbf {Comprehensive Metrics: Single Classifiers.} Comparison of Accuracy, Precision, Recall, and F1-Score for Decision Tree and Naive Bayes}{figure.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Deep Learning: Neural Network}{49}{subsection.6.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Deep Learning with Attention Mechanism}{49}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}3-Class Performance Prediction}{49}{subsection.6.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces \textbf  {Feature Count Effect: Single Classifiers.} Accuracy trends showing how number of features impacts Decision Tree and Naive Bayes performance.}}{50}{figure.6.3}\protected@file@percent }
\newlabel{fig:dt_nb_accuracy_trend}{{6.3}{50}{\textbf {Feature Count Effect: Single Classifiers.} Accuracy trends showing how number of features impacts Decision Tree and Naive Bayes performance}{figure.6.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Binary Classification (Dropout vs Not Dropout)}{50}{subsection.6.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces \textbf  {Ensemble Methods Feature Selection.} Accuracy heatmap for Random Forest, AdaBoost, and XGBoost across all feature selection methods and configurations.}}{51}{figure.6.4}\protected@file@percent }
\newlabel{fig:ensemble_heatmap}{{6.4}{51}{\textbf {Ensemble Methods Feature Selection.} Accuracy heatmap for Random Forest, AdaBoost, and XGBoost across all feature selection methods and configurations}{figure.6.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Explainable AI: SHAP Analysis}{51}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Tree-Based Models: SHAP Importance}{51}{subsection.6.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Comparative SHAP Analysis}{51}{subsection.6.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces \textbf  {Comprehensive Metrics: Ensemble Methods.} Detailed comparison of Accuracy, Precision, Recall, F1-Score, and AUC across ensemble classifiers.}}{52}{figure.6.5}\protected@file@percent }
\newlabel{fig:ensemble_metrics}{{6.5}{52}{\textbf {Comprehensive Metrics: Ensemble Methods.} Detailed comparison of Accuracy, Precision, Recall, F1-Score, and AUC across ensemble classifiers}{figure.6.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Comprehensive Model Evaluation Results}{52}{section.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Performance Metrics: Accuracy, Precision, Recall, F1-Score}{52}{subsection.6.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Comprehensive Performance Metrics for All Models}}{52}{table.6.1}\protected@file@percent }
\newlabel{tab:comprehensive_metrics}{{6.1}{52}{Comprehensive Performance Metrics for All Models}{table.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces \textbf  {Feature Count Effect: Ensemble Methods.} Accuracy trends for ensemble methods showing relative robustness to feature count variations.}}{53}{figure.6.6}\protected@file@percent }
\newlabel{fig:ensemble_accuracy_trend}{{6.6}{53}{\textbf {Feature Count Effect: Ensemble Methods.} Accuracy trends for ensemble methods showing relative robustness to feature count variations}{figure.6.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Confusion Matrices}{53}{subsection.6.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces \textbf  {Ensemble Methods Comparative Performance.} Direct comparison of Random Forest, AdaBoost, and XGBoost across multiple metrics.}}{54}{figure.6.7}\protected@file@percent }
\newlabel{fig:ensemble_comparison}{{6.7}{54}{\textbf {Ensemble Methods Comparative Performance.} Direct comparison of Random Forest, AdaBoost, and XGBoost across multiple metrics}{figure.6.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}ROC Curves and AUC Scores}{54}{subsection.6.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces AUC Scores (Micro-Average) for All Models}}{54}{table.6.2}\protected@file@percent }
\newlabel{tab:auc_scores}{{6.2}{54}{AUC Scores (Micro-Average) for All Models}{table.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces \textbf  {Neural Network Feature Selection Analysis.} Accuracy heatmap across all feature selection methods and feature counts for standard neural network.}}{55}{figure.6.8}\protected@file@percent }
\newlabel{fig:nn_heatmap}{{6.8}{55}{\textbf {Neural Network Feature Selection Analysis.} Accuracy heatmap across all feature selection methods and feature counts for standard neural network}{figure.6.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.4}10-Fold Cross-Validation}{55}{subsection.6.4.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces 10-Fold Cross-Validation Results for All Models}}{55}{table.6.3}\protected@file@percent }
\newlabel{tab:cross_validation}{{6.3}{55}{10-Fold Cross-Validation Results for All Models}{table.6.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces \textbf  {Comprehensive Metrics: Neural Network.} Performance metrics comparison for neural network across different configurations.}}{56}{figure.6.9}\protected@file@percent }
\newlabel{fig:nn_metrics}{{6.9}{56}{\textbf {Comprehensive Metrics: Neural Network.} Performance metrics comparison for neural network across different configurations}{figure.6.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.5}Summary Evaluation Table}{56}{subsection.6.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Model Recommendations}{56}{section.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Best Models by Objective}{56}{subsection.6.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces \textbf  {Feature Count Effect: Neural Network.} Accuracy trends for neural network showing sensitivity to feature dimensionality.}}{57}{figure.6.10}\protected@file@percent }
\newlabel{fig:nn_accuracy_trend}{{6.10}{57}{\textbf {Feature Count Effect: Neural Network.} Accuracy trends for neural network showing sensitivity to feature dimensionality}{figure.6.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Key Academic Insights}{57}{subsection.6.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces \textbf  {Deep Learning Attention Model Training History.} Evolution of accuracy, loss, precision, and recall across 200 epochs, demonstrating convergence and model learning.}}{58}{figure.6.11}\protected@file@percent }
\newlabel{fig:dl_attention_training}{{6.11}{58}{\textbf {Deep Learning Attention Model Training History.} Evolution of accuracy, loss, precision, and recall across 200 epochs, demonstrating convergence and model learning}{figure.6.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Deployment Recommendations}{58}{section.6.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces \textbf  {Confusion Matrix: Deep Learning Attention (3-Class).} Classification results showing per-class performance for Dropout, Enrolled, and Graduate outcomes.}}{59}{figure.6.12}\protected@file@percent }
\newlabel{fig:dl_attention_confusion}{{6.12}{59}{\textbf {Confusion Matrix: Deep Learning Attention (3-Class).} Classification results showing per-class performance for Dropout, Enrolled, and Graduate outcomes}{figure.6.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces \textbf  {Attention Mechanism Feature Importance.} Top 15 features weighted by attention mechanism, showing automatic feature importance discovery.}}{59}{figure.6.13}\protected@file@percent }
\newlabel{fig:dl_attention_importance}{{6.13}{59}{\textbf {Attention Mechanism Feature Importance.} Top 15 features weighted by attention mechanism, showing automatic feature importance discovery}{figure.6.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces \textbf  {SHAP Importance: Decision Tree.} Feature importance based on Shapley values, showing decision tree's feature attribution.}}{60}{figure.6.14}\protected@file@percent }
\newlabel{fig:shap_dt_importance}{{6.14}{60}{\textbf {SHAP Importance: Decision Tree.} Feature importance based on Shapley values, showing decision tree's feature attribution}{figure.6.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces \textbf  {SHAP Summary Plot: Decision Tree.} Distribution of SHAP values showing positive/negative feature impacts on predictions.}}{60}{figure.6.15}\protected@file@percent }
\newlabel{fig:shap_dt_summary}{{6.15}{60}{\textbf {SHAP Summary Plot: Decision Tree.} Distribution of SHAP values showing positive/negative feature impacts on predictions}{figure.6.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.16}{\ignorespaces \textbf  {SHAP Importance: Naive Bayes.} Feature importance analysis for probabilistic classifier.}}{61}{figure.6.16}\protected@file@percent }
\newlabel{fig:shap_nb_importance}{{6.16}{61}{\textbf {SHAP Importance: Naive Bayes.} Feature importance analysis for probabilistic classifier}{figure.6.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.17}{\ignorespaces \textbf  {SHAP Summary Plot: Naive Bayes.} Shapley-based feature impact analysis for Naive Bayes classifier.}}{62}{figure.6.17}\protected@file@percent }
\newlabel{fig:shap_nb_summary}{{6.17}{62}{\textbf {SHAP Summary Plot: Naive Bayes.} Shapley-based feature impact analysis for Naive Bayes classifier}{figure.6.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.18}{\ignorespaces \textbf  {SHAP Importance: Random Forest.} Feature importance from ensemble tree model, showing collective feature contributions.}}{63}{figure.6.18}\protected@file@percent }
\newlabel{fig:shap_rf_importance}{{6.18}{63}{\textbf {SHAP Importance: Random Forest.} Feature importance from ensemble tree model, showing collective feature contributions}{figure.6.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.19}{\ignorespaces \textbf  {SHAP Summary Plot: Random Forest.} Comprehensive feature impact distribution for ensemble model.}}{64}{figure.6.19}\protected@file@percent }
\newlabel{fig:shap_rf_summary}{{6.19}{64}{\textbf {SHAP Summary Plot: Random Forest.} Comprehensive feature impact distribution for ensemble model}{figure.6.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.20}{\ignorespaces \textbf  {SHAP Importance: AdaBoost.} Adaptive boosting feature importance analysis.}}{65}{figure.6.20}\protected@file@percent }
\newlabel{fig:shap_ada_importance}{{6.20}{65}{\textbf {SHAP Importance: AdaBoost.} Adaptive boosting feature importance analysis}{figure.6.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.21}{\ignorespaces \textbf  {SHAP Summary Plot: AdaBoost.} Feature impact distribution for boosted ensemble classifier.}}{66}{figure.6.21}\protected@file@percent }
\newlabel{fig:shap_ada_summary}{{6.21}{66}{\textbf {SHAP Summary Plot: AdaBoost.} Feature impact distribution for boosted ensemble classifier}{figure.6.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.22}{\ignorespaces \textbf  {SHAP Importance: XGBoost.} Extreme gradient boosting feature importance, showing top predictors.}}{67}{figure.6.22}\protected@file@percent }
\newlabel{fig:shap_xgb_importance}{{6.22}{67}{\textbf {SHAP Importance: XGBoost.} Extreme gradient boosting feature importance, showing top predictors}{figure.6.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.23}{\ignorespaces \textbf  {SHAP Summary Plot: XGBoost.} Feature impact analysis for XGBoost, showing SHAP value distributions.}}{68}{figure.6.23}\protected@file@percent }
\newlabel{fig:shap_xgb_summary}{{6.23}{68}{\textbf {SHAP Summary Plot: XGBoost.} Feature impact analysis for XGBoost, showing SHAP value distributions}{figure.6.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.24}{\ignorespaces \textbf  {SHAP Importance: Neural Network.} Feature importance approximation for deep learning model.}}{69}{figure.6.24}\protected@file@percent }
\newlabel{fig:shap_nn_importance}{{6.24}{69}{\textbf {SHAP Importance: Neural Network.} Feature importance approximation for deep learning model}{figure.6.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.25}{\ignorespaces \textbf  {SHAP Summary Plot: Neural Network.} Feature contribution analysis for neural network predictions.}}{70}{figure.6.25}\protected@file@percent }
\newlabel{fig:shap_nn_summary}{{6.25}{70}{\textbf {SHAP Summary Plot: Neural Network.} Feature contribution analysis for neural network predictions}{figure.6.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.26}{\ignorespaces \textbf  {Cross-Model Feature Importance Comparison.} SHAP feature importance across all 7 models, showing consensus on key predictors.}}{71}{figure.6.26}\protected@file@percent }
\newlabel{fig:shap_comparison}{{6.26}{71}{\textbf {Cross-Model Feature Importance Comparison.} SHAP feature importance across all 7 models, showing consensus on key predictors}{figure.6.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.27}{\ignorespaces \textbf  {Model Accuracy Comparison from SHAP Analysis.} Comprehensive accuracy comparison showing Deep Learning Attention as top performer.}}{71}{figure.6.27}\protected@file@percent }
\newlabel{fig:shap_accuracy}{{6.27}{71}{\textbf {Model Accuracy Comparison from SHAP Analysis.} Comprehensive accuracy comparison showing Deep Learning Attention as top performer}{figure.6.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.28}{\ignorespaces \textbf  {Comprehensive Metrics Comparison.} Multi-panel visualization showing (a) Accuracy/Precision/Recall/F1, (b) AUC scores, (c) Cross-validation accuracy, (d) Feature count vs performance trade-offs.}}{72}{figure.6.28}\protected@file@percent }
\newlabel{fig:comprehensive_metrics}{{6.28}{72}{\textbf {Comprehensive Metrics Comparison.} Multi-panel visualization showing (a) Accuracy/Precision/Recall/F1, (b) AUC scores, (c) Cross-validation accuracy, (d) Feature count vs performance trade-offs}{figure.6.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.29}{\ignorespaces \textbf  {Confusion Matrices: All Models.} Side-by-side comparison of confusion matrices showing true vs predicted labels for all 6 models across three classes (Dropout, Enrolled, Graduate).}}{73}{figure.6.29}\protected@file@percent }
\newlabel{fig:confusion_matrices}{{6.29}{73}{\textbf {Confusion Matrices: All Models.} Side-by-side comparison of confusion matrices showing true vs predicted labels for all 6 models across three classes (Dropout, Enrolled, Graduate)}{figure.6.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.30}{\ignorespaces \textbf  {ROC Curves: All Models.} Receiver Operating Characteristic curves for all models showing per-class and micro-average AUC scores for three-class classification.}}{73}{figure.6.30}\protected@file@percent }
\newlabel{fig:roc_curves}{{6.30}{73}{\textbf {ROC Curves: All Models.} Receiver Operating Characteristic curves for all models showing per-class and micro-average AUC scores for three-class classification}{figure.6.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.31}{\ignorespaces \textbf  {10-Fold Cross-Validation Results.} Distribution of validation scores across 10 folds: (a) Boxplots showing score ranges, (b) Mean accuracy with confidence intervals.}}{74}{figure.6.31}\protected@file@percent }
\newlabel{fig:cross_validation}{{6.31}{74}{\textbf {10-Fold Cross-Validation Results.} Distribution of validation scores across 10 folds: (a) Boxplots showing score ranges, (b) Mean accuracy with confidence intervals}{figure.6.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.32}{\ignorespaces \textbf  {Comprehensive Model Evaluation Summary.} Master comparison table integrating all evaluation metrics, feature counts, and key performance indicators.}}{74}{figure.6.32}\protected@file@percent }
\newlabel{fig:summary_table}{{6.32}{74}{\textbf {Comprehensive Model Evaluation Summary.} Master comparison table integrating all evaluation metrics, feature counts, and key performance indicators}{figure.6.32}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion and Future Work}{75}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Summary of Key Findings}{75}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Deep Learning Performance Achievements}{75}{subsection.7.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Theoretical Framework Validation}{76}{subsection.7.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.3}LLM Integration Success}{76}{subsection.7.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Research Contributions}{76}{section.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Methodological Contributions}{76}{subsection.7.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Empirical Contributions}{77}{subsection.7.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.3}Practical Contributions}{77}{subsection.7.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Limitations and Future Considerations}{77}{section.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Data Limitations}{77}{subsection.7.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Methodological Limitations}{77}{subsection.7.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}Generalization Considerations}{78}{subsection.7.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Implications for Educational Practice}{78}{section.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.1}Early Warning System Implementation}{78}{subsection.7.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}Evidence-Based Retention Policy}{78}{subsection.7.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.3}Equity and Fairness Considerations}{79}{subsection.7.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Future Research Directions}{79}{section.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Methodological Extensions}{79}{subsection.7.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Data and Evaluation Extensions}{79}{subsection.7.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.3}Deployment and Implementation Research}{80}{subsection.7.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.4}Domain-Specific Enhancements}{80}{subsection.7.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Concluding Remarks}{80}{section.7.6}\protected@file@percent }
\bibstyle{unsrt}
\bibdata{fydp}
\@writefile{toc}{\contentsline {section}{\numberline {7.7}Final Recommendations}{81}{section.7.7}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{References}{81}{Item.147}\protected@file@percent }
\gdef \@abspage@last{95}
