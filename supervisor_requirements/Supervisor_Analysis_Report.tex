\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{multicol}

% Page setup
\geometry{margin=2.5cm}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\leftmark}
\fancyhead[R]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% Section formatting
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue,
    pdftitle={Student Dropout Prediction - Comprehensive Analysis},
    pdfauthor={},
}

% Define colors
\definecolor{headercolor}{RGB}{70,130,180}
\definecolor{lightgray}{RGB}{240,240,240}

\begin{document}

% Title Page
\begin{titlepage}
    \centering
    \vspace*{2cm}
    
    {\Huge\bfseries Student Dropout Prediction\par}
    \vspace{0.5cm}
    {\LARGE Comprehensive Data Analysis Report\par}
    \vspace{2cm}
    
    {\Large\itshape Supervisor Requirements Analysis\par}
    \vspace{3cm}
    
    {\large\bfseries Dataset Overview and Modeling Results\par}
    \vspace{0.5cm}
    {\large 4,424 Students | 34 Features | 3 Classes | 7 Models\par}
    \vspace{2cm}
    
    {\large December 2025\par}
    
    \vfill
    
    {\large European Higher Education Institution\par}
\end{titlepage}

\tableofcontents
\newpage

% Executive Summary
\section{Executive Summary}

This comprehensive report presents a detailed analysis of student dropout prediction in higher education, addressing all requirements specified by the thesis supervisor. The analysis encompasses dataset exploration, feature engineering, feature selection optimization, multiple machine learning models, explainable AI techniques, and rigorous evaluation metrics.

\subsection{Key Findings}

\begin{itemize}
    \item \textbf{Dataset}: 4,424 students with 34 features across academic, financial, and demographic categories
    \item \textbf{Class Distribution}: Dropout (32.1\%), Enrolled (17.9\%), Graduate (49.9\%)
    \item \textbf{Best Overall Model}: Random Forest achieving 76.72\% test accuracy with 91.36\% AUC
    \item \textbf{Best Cross-Validation}: XGBoost with 78.21\% mean CV accuracy
    \item \textbf{Top Predictors}: Curricular units approved (both semesters), tuition fees, and semester grades
    \item \textbf{Feature Selection}: Optimized from 34 to 10-30 features depending on model type
    \item \textbf{Explainable AI}: SHAP analysis completed for all 7 models
    \item \textbf{Deep Learning}: Attention-based model achieving 76.61\% (3-class) and 87.23\% (binary)
\end{itemize}

\subsection{Supervisor Requirements Coverage}

\begin{table}[H]
\centering
\caption{Analysis Coverage of Supervisor Requirements}
\begin{tabular}{clc}
\toprule
\textbf{Req.} & \textbf{Description} & \textbf{Status} \\
\midrule
1-3 & Dataset Overview (Students, Features, Classes) & $\checkmark$ \\
4-6 & Feature Lists (Academic, Financial, Demographic) & $\checkmark$ \\
7 & Feature Ranking & $\checkmark$ \\
8 & Dropout Feature Importance & $\checkmark$ \\
9 & Multi-Model Classification (6 models) & $\checkmark$ \\
10 & Explainable AI (SHAP for all models) & $\checkmark$ \\
11.1 & Accuracy, Precision, Recall, F1-Score & $\checkmark$ \\
11.2 & Confusion Matrices & $\checkmark$ \\
11.3 & ROC Curves \& AUC & $\checkmark$ \\
11.4 & 10-Fold Cross-Validation & $\checkmark$ \\
\bottomrule
\end{tabular}
\end{table}

\newpage

% Section 1: Dataset Overview
\section{Dataset Overview}

\subsection{Total Students and Features}

The dataset contains comprehensive information about \textbf{4,424 students} enrolled in various degree programs at a European higher education institution. The analysis focuses on predicting student outcomes across three classes:

\begin{itemize}
    \item \textbf{Dropout}: 1,421 students (32.1\%)
    \item \textbf{Enrolled}: 794 students (17.9\%)
    \item \textbf{Graduate}: 2,209 students (49.9\%)
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{outputs/figures/01_class_distribution.png}
    \caption{Distribution of student outcomes across three classes}
    \label{fig:class_distribution}
\end{figure}

\subsection{Feature Categories}

The dataset comprises \textbf{46 features} organized into three main categories:

\begin{table}[H]
\centering
\caption{Feature Categories and Counts}
\label{tab:feature_categories}
\begin{tabular}{lc}
\toprule
\textbf{Category} & \textbf{Number of Features} \\
\midrule
Academic Features & 18 \\
Financial Features & 12 \\
Demographic Features & 16 \\
\midrule
\textbf{Total} & \textbf{46} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Note}: Some features appear in multiple categories (e.g., Gender, Age at enrollment appear in both Financial and Demographic categories), resulting in 46 total features when counted separately.

\newpage

% Section 2: Feature Lists
\section{Feature Lists}

\subsection{Academic Features (18 features)}

Academic features capture student performance, enrollment patterns, and qualifications:

\begin{multicols}{2}
\begin{enumerate}
    \item Curricular units 1st sem (credited)
    \item Curricular units 1st sem (enrolled)
    \item Curricular units 1st sem (evaluations)
    \item Curricular units 1st sem (approved)
    \item Curricular units 1st sem (grade)
    \item Curricular units 1st sem (without evaluations)
    \item Curricular units 2nd sem (credited)
    \item Curricular units 2nd sem (enrolled)
    \item Curricular units 2nd sem (evaluations)
    \item Curricular units 2nd sem (approved)
    \item Curricular units 2nd sem (grade)
    \item Curricular units 2nd sem (without evaluations)
    \item Previous qualification grade
    \item Admission grade
    \item Application mode
    \item Application order
    \item Course
    \item Daytime/evening attendance
\end{enumerate}
\end{multicols}

\subsection{Financial Features (12 features)}

Financial features include tuition status, scholarships, and economic indicators:

\begin{multicols}{2}
\begin{enumerate}
    \item Tuition fees up to date
    \item Scholarship holder
    \item Debtor
    \item Unemployment rate
    \item Inflation rate
    \item GDP
    \item International
    \item Displaced
    \item Educational special needs
    \item Gender
    \item Age at enrollment
    \item Nacionality
\end{enumerate}
\end{multicols}

\subsection{Demographic Features (16 features)}

Demographic features capture personal and family background including marital status, parent qualifications and occupations, gender, age, nationality, and special needs status:

\begin{multicols}{2}
\begin{enumerate}
    \item Marital status
    \item Previous qualification
    \item Mothers qualification
    \item Fathers qualification
    \item Mothers occupation
    \item Fathers occupation
    \item Gender
    \item Age at enrollment
    \item International
    \item Displaced
    \item Educational special needs
    \item Debtor
    \item Tuition fees up to date
    \item Scholarship holder
    \item Nacionality
    \item Application mode
\end{enumerate}
\end{multicols}

\newpage

% Section 3: Feature Ranking
\section{Feature Ranking}

Five different feature ranking methods were applied to identify the most important predictors. Figure~\ref{fig:ranking_heatmap} shows how different methods rank the top features.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/03_ranking_heatmap.png}
    \caption{Feature ranking heatmap comparing all five methods for top 20 features}
    \label{fig:ranking_heatmap}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{outputs/figures/03_top20_information_gain.png}
    \caption{Top 20 features ranked by Information Gain}
    \label{fig:ig_ranking}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{outputs/figures/03_top20_gini_importance.png}
    \caption{Top 20 features ranked by Gini importance}
    \label{fig:gini_ranking}
\end{figure}

\textbf{Key Finding}: Curricular units 2nd semester (approved) and tuition fees status consistently rank in the top 3 across all methods.

\newpage

% Section 4: Dropout Feature Importance
\section{Dropout Feature Importance}

A focused analysis identified the most influential features for predicting student dropout using four complementary methods.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/04_top20_dropout_features.png}
    \caption{Top 20 features for dropout prediction (composite score from 4 methods)}
    \label{fig:dropout_top20}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/04_methods_comparison.png}
    \caption{Comparison of four feature importance methods for dropout prediction}
    \label{fig:dropout_methods}
\end{figure}

\textbf{Top 5 Dropout Predictors}:
\begin{enumerate}
    \item Curricular units 2nd sem (approved)
    \item Curricular units 2nd sem (grade)
    \item Tuition fees up to date
    \item Curricular units 1st sem (approved)
    \item Curricular units 1st sem (grade)
\end{enumerate}

\newpage

% Section 5: Feature Selection Optimization
\section{Feature Selection Optimization}

Comprehensive feature selection was performed for all 6 models using 9 different methods to identify optimal feature subsets.

\subsection{Single Classifiers: Decision Tree \& Naive Bayes}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/08_accuracy_heatmap.png}
    \caption{Accuracy heatmap for Decision Tree and Naive Bayes across all feature selection methods and feature counts}
    \label{fig:dt_nb_heatmap}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/08_all_metrics_comparison.png}
    \caption{Comprehensive metrics comparison for Decision Tree and Naive Bayes}
    \label{fig:dt_nb_metrics}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/08_accuracy_vs_features.png}
    \caption{Accuracy trends vs. number of features for Decision Tree and Naive Bayes}
    \label{fig:dt_nb_accuracy_trend}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{outputs/figures/08_best_accuracy_per_method.png}
    \caption{Best accuracy achieved by each feature selection method for Decision Tree and Naive Bayes}
    \label{fig:dt_nb_best_accuracy}
\end{figure}

\textbf{Best Configurations}:
\begin{itemize}
    \item Decision Tree: Information Gain, 10 features, 68.81\% accuracy
    \item Naive Bayes: Information Gain, 15 features, 72.66\% accuracy
\end{itemize}

\newpage

\subsection{Ensemble Methods: Random Forest, AdaBoost, XGBoost}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/09_ensemble_accuracy_heatmap.png}
    \caption{Accuracy heatmap for ensemble methods across all feature selection configurations}
    \label{fig:ensemble_heatmap}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/09_ensemble_all_metrics_comparison.png}
    \caption{Comprehensive metrics comparison for ensemble methods}
    \label{fig:ensemble_metrics}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{outputs/figures/09_ensemble_best_accuracy_per_method.png}
    \caption{Best accuracy achieved by each ensemble method across different feature selection techniques}
    \label{fig:ensemble_best}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/09_ensemble_accuracy_vs_features.png}
    \caption{Accuracy trends vs. number of features for ensemble methods}
    \label{fig:ensemble_accuracy_trend}
\end{figure}

\textbf{Best Configurations}:
\begin{itemize}
    \item Random Forest: RFE, 20 features, 77.85\% accuracy
    \item AdaBoost: Mutual Information, 15 features, 77.06\% accuracy  
    \item XGBoost: RF Importance, 30 features, 77.97\% accuracy
\end{itemize}

\newpage

\subsection{Deep Learning: Neural Network}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/10_nn_accuracy_heatmap.png}
    \caption{Accuracy heatmap for Neural Network across all feature selection methods and feature counts}
    \label{fig:nn_heatmap}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/10_nn_all_metrics_comparison.png}
    \caption{Comprehensive metrics comparison for Neural Network}
    \label{fig:nn_metrics}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{outputs/figures/10_nn_accuracy_vs_features.png}
    \caption{Neural Network accuracy vs number of features for all selection methods}
    \label{fig:nn_vs_features}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{outputs/figures/10_nn_best_accuracy_per_method.png}
    \caption{Best accuracy achieved by each feature selection method for Neural Network}
    \label{fig:nn_best_accuracy}
\end{figure}

\textbf{Best Configuration}:
\begin{itemize}
    \item Neural Network: ANOVA F-statistic, 15 features, 76.84\% accuracy
\end{itemize}

\newpage

\subsection{Deep Learning with Attention Mechanism}

\subsubsection{3-Class Classification (Dropout/Enrolled/Graduate)}

The Deep Learning Attention model uses a self-attention mechanism to automatically weight feature importance during prediction.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/13_deep_learning_attention_training.png}
    \caption{Deep Learning Attention model training history showing accuracy, loss, precision, and recall over 200 epochs}
    \label{fig:dl_attention_training}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{outputs/figures/13_deep_learning_attention_confusion_matrix.png}
    \caption{Confusion matrix for Deep Learning Attention model (3-class classification)}
    \label{fig:dl_attention_confusion}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{outputs/figures/13_deep_learning_attention_importance.png}
    \caption{Top 15 feature importance from Deep Learning Attention model}
    \label{fig:dl_attention_importance}
\end{figure}

\textbf{3-Class Performance}:
\begin{itemize}
    \item Test Accuracy: 76.61\%
    \item Architecture: 64 → Attention → 32 → 16 → 3 neurons (Softmax)
    \item Features: 20 (ANOVA F-test selection)
    \item Per-Class Recall: Dropout 76\%, Enrolled 38\%, Graduate 90\%
\end{itemize}

\subsubsection{Binary Classification (Dropout vs Not Dropout)}

Following the journal methodology, we also implemented binary dropout prediction achieving state-of-the-art performance.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{outputs/plots/14_binary_confusion_matrix.png}
    \caption{Binary DPN-A confusion matrix (Dropout vs Not Dropout)}
    \label{fig:binary_confusion}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/plots/14_binary_roc_curve.png}
    \caption{ROC curve for binary dropout prediction (AUC = 0.9301)}
    \label{fig:binary_roc}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{outputs/plots/14_binary_feature_importance.png}
    \caption{Top 15 features for binary dropout prediction from attention weights}
    \label{fig:binary_importance}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/plots/14_binary_precision_recall.png}
    \caption{Precision-Recall curve for binary dropout prediction}
    \label{fig:binary_pr}
\end{figure}

\textbf{Binary Classification Performance}:
\begin{itemize}
    \item \textbf{Test Accuracy}: 87.23\% (Journal target: 87.05\%) $\checkmark$
    \item \textbf{AUC-ROC}: 0.9301 (Journal target: 0.9100) $\checkmark$
    \item \textbf{F1-Score}: 0.7919
    \item Architecture: 64 → Attention → 32 → 16 → 1 neuron (Sigmoid)
    \item Features: ALL 34 features (no selection)
    \item Class Weights: \{0: 0.74, 1: 1.56\} for imbalance handling
    \item Dropout Detection: 75.7\% recall, 83.0\% precision
    \item Not Dropout: 92.7\% recall, 89.0\% precision
\end{itemize}

\textbf{Key Insight}: Binary classification achieves 10.6\% higher accuracy than 3-class (87.23\% vs 76.61\%) due to simpler decision boundary. Binary is ideal for early dropout warning systems, while 3-class provides richer outcome forecasting.

\newpage

% Section 6: Explainable AI (SHAP Analysis)
\section{Explainable AI - SHAP Analysis}

SHAP (SHapley Additive exPlanations) analysis was performed on all 7 models to provide complete transparency into model predictions.

\subsection{Decision Tree SHAP}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{outputs/figures/11_shap_decision_tree_importance.png}
    \caption{SHAP feature importance for Decision Tree (10 features)}
    \label{fig:shap_dt_importance}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/11_shap_decision_tree_summary.png}
    \caption{SHAP summary plot for Decision Tree showing feature impact distribution}
    \label{fig:shap_dt_summary}
\end{figure}

\newpage

\subsection{Naive Bayes SHAP}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{outputs/figures/11_shap_naive_bayes_importance.png}
    \caption{SHAP feature importance for Naive Bayes (15 features)}
    \label{fig:shap_nb_importance}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/11_shap_naive_bayes_summary.png}
    \caption{SHAP summary plot for Naive Bayes}
    \label{fig:shap_nb_summary}
\end{figure}

\newpage

\subsection{Random Forest SHAP}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{outputs/figures/11_shap_random_forest_importance.png}
    \caption{SHAP feature importance for Random Forest (20 features)}
    \label{fig:shap_rf_importance}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/11_shap_random_forest_summary.png}
    \caption{SHAP summary plot for Random Forest}
    \label{fig:shap_rf_summary}
\end{figure}

\newpage

\subsection{AdaBoost SHAP}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{outputs/figures/11_shap_adaboost_importance.png}
    \caption{SHAP feature importance for AdaBoost (15 features)}
    \label{fig:shap_ada_importance}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/11_shap_adaboost_summary.png}
    \caption{SHAP summary plot for AdaBoost}
    \label{fig:shap_ada_summary}
\end{figure}

\newpage

\subsection{XGBoost SHAP}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{outputs/figures/11_shap_xgboost_importance.png}
    \caption{SHAP feature importance for XGBoost (30 features)}
    \label{fig:shap_xgb_importance}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/11_shap_xgboost_summary.png}
    \caption{SHAP summary plot for XGBoost}
    \label{fig:shap_xgb_summary}
\end{figure}

\newpage

\subsection{Neural Network SHAP}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{outputs/figures/11_shap_neural_network_importance.png}
    \caption{SHAP feature importance for Neural Network (15 features)}
    \label{fig:shap_nn_importance}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/11_shap_neural_network_summary.png}
    \caption{SHAP summary plot for Neural Network}
    \label{fig:shap_nn_summary}
\end{figure}

\newpage

\subsection{Comparative SHAP Analysis}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/11_all_models_feature_importance_comparison.png}
    \caption{SHAP feature importance comparison across all 7 models}
    \label{fig:shap_comparison}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{outputs/figures/11_all_models_accuracy_comparison.png}
    \caption{Model accuracy comparison from SHAP analysis}
    \label{fig:shap_accuracy}
\end{figure}

\textbf{Key Insight}: While different models use different feature subsets (10-34 features), curricular units approved and tuition fees consistently emerge as top predictors across all models. The Deep Learning Attention model uses its attention mechanism to automatically learn feature importance, achieving competitive results with 20 selected features.

\newpage

% Section 7: Comprehensive Model Evaluation
\section{Comprehensive Model Evaluation}

\subsection{11.1 Accuracy, Precision, Recall, F1-Score}

\begin{table}[H]
\centering
\caption{Comprehensive Performance Metrics for All Models}
\label{tab:comprehensive_metrics}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
\midrule
Decision Tree & 0.6701 & 0.6702 & 0.6701 & 0.6701 \\
Naive Bayes & 0.7085 & 0.6856 & 0.7085 & 0.6848 \\
\rowcolor{lightgray}
\textbf{Random Forest} & \textbf{0.7672} & \textbf{0.7540} & \textbf{0.7672} & \textbf{0.7561} \\
AdaBoost & 0.7424 & 0.7254 & 0.7424 & 0.7308 \\
XGBoost & 0.7593 & 0.7526 & 0.7593 & 0.7544 \\
Neural Network & 0.7141 & 0.7064 & 0.7141 & 0.7100 \\
DL Attention & 0.7661 & 0.7616 & 0.7661 & 0.7638 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/12_comprehensive_metrics_comparison.png}
    \caption{Comprehensive metrics comparison: (a) Accuracy/Precision/Recall/F1, (b) AUC, (c) CV Accuracy, (d) Features vs Accuracy}
    \label{fig:comprehensive_metrics}
\end{figure}

\newpage

\subsection{11.2 Confusion Matrices}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/12_all_models_confusion_matrices.png}
    \caption{Confusion matrices for all 6 models showing true vs predicted labels}
    \label{fig:confusion_matrices}
\end{figure}

\textbf{Analysis}: Random Forest and XGBoost show the most balanced performance across all three classes with minimal confusion between Dropout and Graduate predictions.

\newpage

\subsection{11.3 ROC Curves and AUC Scores}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/12_all_models_roc_curves.png}
    \caption{ROC curves for all 6 models with per-class and micro-average AUC scores}
    \label{fig:roc_curves}
\end{figure}

\begin{table}[H]
\centering
\caption{AUC Scores (Micro-Average) for All Models}
\label{tab:auc_scores}
\begin{tabular}{lc}
\toprule
\textbf{Model} & \textbf{Micro-Average AUC} \\
\midrule
Decision Tree & 0.7581 \\
Naive Bayes & 0.8434 \\
\rowcolor{lightgray}
\textbf{Random Forest} & \textbf{0.9136} \\
AdaBoost & 0.8896 \\
XGBoost & 0.9133 \\
Neural Network & 0.8608 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Finding}: Both Random Forest and XGBoost achieve excellent AUC scores above 0.91, indicating strong discriminative ability across all three classes.

\newpage

\subsection{11.4 10-Fold Cross-Validation}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/12_cross_validation_results.png}
    \caption{10-fold cross-validation results: (a) Score distribution boxplot, (b) Mean accuracy with error bars}
    \label{fig:cross_validation}
\end{figure}

\begin{table}[H]
\centering
\caption{10-Fold Cross-Validation Results for All Models}
\label{tab:cross_validation}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Mean Accuracy} & \textbf{Std Dev} & \textbf{Min} & \textbf{Max} \\
\midrule
Decision Tree & 0.6747 & 0.0130 & 0.6569 & 0.7059 \\
Naive Bayes & 0.7247 & 0.0207 & 0.6923 & 0.7557 \\
Random Forest & 0.7722 & 0.0124 & 0.7489 & 0.7941 \\
AdaBoost & 0.7439 & 0.0117 & 0.7195 & 0.7624 \\
\rowcolor{lightgray}
\textbf{XGBoost} & \textbf{0.7821} & \textbf{0.0081} & \textbf{0.7692} & \textbf{0.7964} \\
Neural Network & 0.7233 & 0.0149 & 0.7043 & 0.7579 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Finding}: XGBoost demonstrates the most stable and highest cross-validation performance with 78.21\% mean accuracy and lowest standard deviation (0.81\%), indicating robust generalization.

\newpage

\subsection{Summary Evaluation Table}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{outputs/figures/12_model_evaluation_summary_table.png}
    \caption{Comprehensive summary table with all evaluation metrics}
    \label{fig:summary_table}
\end{figure}

\newpage

% Section 8: Conclusions and Recommendations
\section{Conclusions and Recommendations}

\subsection{Overall Best Models}

Based on comprehensive evaluation across multiple metrics:

\begin{enumerate}
    \item \textbf{Best Test Accuracy (3-Class)}: Random Forest (76.72\%)
    \item \textbf{Best AUC Score}: Random Forest (0.9136)
    \item \textbf{Best Cross-Validation}: XGBoost (78.21\%)
    \item \textbf{Most Stable}: XGBoost (CV Std = 0.0081)
    \item \textbf{Best Binary Dropout Detection}: DL Attention (87.23\%, AUC 0.9301)
\end{enumerate}

\subsection{Key Academic Insights}

\begin{enumerate}
    \item \textbf{Academic Performance Dominates}: Curricular units approved and grades in both semesters are consistently the strongest predictors across all models and analyses.
    
    \item \textbf{Financial Status Matters}: Tuition payment status ranks in top 3-5 features across all methods, indicating financial difficulties are a major dropout risk factor.
    
    \item \textbf{First Semester is Critical}: Performance in the first semester strongly predicts final outcomes, suggesting early intervention opportunities.
    
    \item \textbf{Feature Selection Improves Performance}: Reducing from 34 to 10-30 optimally selected features maintains or improves accuracy while reducing complexity.
    
    \item \textbf{Ensemble Methods Excel}: Tree-based ensemble methods (Random Forest, XGBoost) significantly outperform single classifiers, achieving 76-78\% accuracy vs 67-71\%.
    
    \item \textbf{Binary vs Multi-Class Trade-off}: Binary dropout prediction achieves 87.23\% accuracy (DL Attention) compared to 76.61\% for 3-class prediction, demonstrating the inherent difficulty of multi-class student outcome forecasting.
    
    \item \textbf{Attention Mechanism Value}: The self-attention mechanism automatically learns feature importance weights, achieving competitive performance while providing interpretability through attention weights.
\end{enumerate}

\subsection{Recommendations for Deployment}

\textbf{For 3-Class Outcome Prediction}: Use \textbf{XGBoost} as the primary model due to its highest cross-validation performance (78.21\%) and most stable predictions (lowest variance).

\textbf{For Binary Dropout Detection}: Use \textbf{Deep Learning Attention (DPN-A)} model achieving 87.23\% accuracy and 0.9301 AUC-ROC, exceeding journal benchmarks. This is ideal for early warning systems requiring high accuracy in identifying at-risk students.

\textbf{Hybrid Approach}: Deploy both models - use binary DL model for high-accuracy early alerts, and 3-class XGBoost for comprehensive outcome forecasting and academic planning.

\newpage

% Appendix
\appendix

\section{Technical Details}

\subsection{Computational Environment}

\begin{itemize}
    \item \textbf{Python Version}: 3.10+
    \item \textbf{Core Libraries}: scikit-learn, xgboost, pandas, numpy, matplotlib, seaborn
    \item \textbf{Explainability}: SHAP 0.43+
    \item \textbf{Hardware}: Standard CPU (no GPU required)
\end{itemize}

\subsection{Data Preprocessing}

\begin{itemize}
    \item \textbf{Missing Values}: None detected (complete dataset)
    \item \textbf{Target Encoding}: Dropout=0, Enrolled=1, Graduate=2
    \item \textbf{Feature Scaling}: StandardScaler for Neural Network only
    \item \textbf{Train/Test Split}: 80/20 stratified (3,539/885 samples)
    \item \textbf{Cross-Validation}: Stratified 10-fold with shuffle
    \item \textbf{Random Seed}: 42 (for reproducibility)
\end{itemize}

\subsection{Optimal Model Configurations}

\begin{itemize}
    \item \textbf{Decision Tree}: Information Gain selection, 10 features
    \item \textbf{Naive Bayes}: Information Gain selection, 15 features
    \item \textbf{Random Forest}: RFE selection, 20 features
    \item \textbf{AdaBoost}: Mutual Info selection, 15 features
    \item \textbf{XGBoost}: RF Importance selection, 30 features
    \item \textbf{Neural Network}: ANOVA F-stat selection, 15 features
    \item \textbf{DL Attention (3-class)}: ANOVA F-stat selection, 20 features
    \item \textbf{DL Attention (binary)}: No selection, ALL 34 features
\end{itemize}

\section{Generated Outputs Summary}

\subsection{Visualizations Generated}

\textbf{Total Figures}: 58 visualizations across all analyses

\begin{itemize}
    \item Dataset Overview: 1 figure
    \item Feature Ranking: 3 figures
    \item Dropout Analysis: 2 figures
    \item Feature Selection: 15 figures
    \item Deep Learning Attention: 7 figures (3-class + binary)
    \item SHAP Analysis: 16 figures (includes DL Attention)
    \item Model Evaluation: 5 figures
    \item Summary Visualizations: 9 figures
\end{itemize}

\end{document}