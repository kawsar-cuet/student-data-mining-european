\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{elsarticle-harv}
\Newlabel{cor1}{1}
\Newlabel{inst1}{a}
\citation{Tinto1993}
\citation{Romero2020}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Research Contributions}{3}{subsection.1.1}\protected@file@percent }
\citation{Tinto1993}
\citation{Bean1985}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Theoretical Framework}{4}{subsection.1.2}\protected@file@percent }
\citation{Romero2020}
\citation{Kotsiantis2013}
\citation{Asif2017}
\@writefile{toc}{\contentsline {section}{\numberline {2}Literature Review}{5}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Educational Data Mining for Student Success}{5}{subsection.2.1}\protected@file@percent }
\citation{Huang2020}
\citation{Adnan2021}
\citation{Vaswani2017}
\citation{Yang2021}
\citation{Wang2022}
\citation{Ruder2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Attention Mechanisms in Educational Contexts}{6}{subsection.2.2}\protected@file@percent }
\citation{Liu2019}
\citation{Chen2020}
\citation{OpenAI2023}
\citation{Martinez2023}
\citation{Nguyen2024}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Multi-Task Learning for Related Educational Outcomes}{7}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Large Language Models for Educational Recommendations}{7}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Research Gaps}{8}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Deep Learning Techniques for Student Outcome Prediction}{8}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Feedforward Neural Networks}{8}{subsection.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison with Recent Literature on Student Outcome Prediction}}{9}{table.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:literature_comparison}{{1}{9}{Comparison with Recent Literature on Student Outcome Prediction}{table.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Attention Mechanisms for Feature Importance}{10}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Multi-Task Learning Architectures}{11}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Dataset and Experimental Methodology}{11}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Dataset Description and Characteristics}{11}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Feature Categories and Attributes}{12}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Dataset Characteristics and Distribution}}{13}{table.caption.2}\protected@file@percent }
\newlabel{tab:dataset_characteristics}{{2}{13}{Dataset Characteristics and Distribution}{table.caption.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Feature categories and theoretical alignment.}}{14}{table.caption.3}\protected@file@percent }
\newlabel{tab:feature_categories}{{3}{14}{Feature categories and theoretical alignment}{table.caption.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Feature Distribution Across Theoretical Frameworks}}{15}{table.caption.4}\protected@file@percent }
\newlabel{tab:framework_distribution}{{4}{15}{Feature Distribution Across Theoretical Frameworks}{table.caption.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Feature Engineering Strategy}{15}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Data Quality and Validation}{17}{subsubsection.4.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Ethical Considerations}{18}{subsubsection.4.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Experimental Methodology and Workflow}{18}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Data Preprocessing Pipeline}{18}{subsubsection.4.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Complete Research Methodology Flowchart (9-Phase Workflow).} Comprehensive visualization of the end-to-end research methodology from data collection through deployment: Phase 1 (Data Collection) acquires 4,424 student records with 46 features; Phase 2 (Preprocessing) implements 8-step pipeline including imputation, encoding, normalization, and tensor conversion; Phase 3 (Theoretical Framework) maps features to Tinto (68\%) and Bean (32\%) models; Phase 4 (Model Development) branches into three architectures (PPN for 3-class performance, DPN-A for binary dropout with attention, HMTL for multi-task learning); Phase 5 (Training \& Optimization) employs Adam optimizer with 10-fold cross-validation and early stopping; Phase 6 (Evaluation) assesses performance using 8 metrics (accuracy, F1, precision, recall, AUC-ROC, AUC-PR, confusion matrices, CV); Phase 7 (Results \& Analysis) validates PPN (76.4\%), DPN-A (87.05\%, AUC-ROC 0.910), and HMTL performance; Phase 8 (LLM Integration) generates GPT-4 powered personalized interventions with rule-based fallback; Phase 9 (Deployment) implements institutional early warning system with advisor dashboard. Color-coded phases facilitate navigation: blue (data), yellow (preprocessing), purple (theory/LLM), green (models), orange (training), teal (evaluation), pink (results), light green (deployment). This systematic methodology ensures reproducibility and addresses both prediction accuracy and actionable intervention generation.}}{19}{figure.caption.7}\protected@file@percent }
\newlabel{fig:methodology_main}{{1}{19}{\textbf {Complete Research Methodology Flowchart (9-Phase Workflow).} Comprehensive visualization of the end-to-end research methodology from data collection through deployment: Phase 1 (Data Collection) acquires 4,424 student records with 46 features; Phase 2 (Preprocessing) implements 8-step pipeline including imputation, encoding, normalization, and tensor conversion; Phase 3 (Theoretical Framework) maps features to Tinto (68\%) and Bean (32\%) models; Phase 4 (Model Development) branches into three architectures (PPN for 3-class performance, DPN-A for binary dropout with attention, HMTL for multi-task learning); Phase 5 (Training \& Optimization) employs Adam optimizer with 10-fold cross-validation and early stopping; Phase 6 (Evaluation) assesses performance using 8 metrics (accuracy, F1, precision, recall, AUC-ROC, AUC-PR, confusion matrices, CV); Phase 7 (Results \& Analysis) validates PPN (76.4\%), DPN-A (87.05\%, AUC-ROC 0.910), and HMTL performance; Phase 8 (LLM Integration) generates GPT-4 powered personalized interventions with rule-based fallback; Phase 9 (Deployment) implements institutional early warning system with advisor dashboard. Color-coded phases facilitate navigation: blue (data), yellow (preprocessing), purple (theory/LLM), green (models), orange (training), teal (evaluation), pink (results), light green (deployment). This systematic methodology ensures reproducibility and addresses both prediction accuracy and actionable intervention generation}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Research Objectives Breakdown with Dual-Task Analysis.} Hierarchical decomposition of the main research question ``Can deep learning + LLM predict student outcomes and provide actionable interventions?'' into two parallel objectives: Objective 1 (Student Performance Prediction) addresses 3-class classification through 5 systematic sub-tasks from baseline model development (Task 1.1: PPN) through multi-metric evaluation (Task 1.5), achieving 76.4\% accuracy with interpretable attention-based predictions; Objective 2 (Student Dropout Prediction) implements binary classification via 5 sub-tasks including attention-based architecture (Task 2.1: DPN-A), temporal feature engineering, and ROC-AUC optimization, achieving superior 87.05\% accuracy with 0.910 AUC-ROC. The integrated multi-task learning analysis (bottom) reveals task interference phenomenon (67.9\% dropout accuracy in HMTL vs 87.05\% in dedicated DPN-A), validating single-task model superiority for this dataset. The LLM enhancement layer (purple box) demonstrates how GPT-4 integration transforms statistical predictions into personalized, actionable student support recommendations. This dual-objective framework addresses both institutional needs: comprehensive student outcome categorization (performance) and targeted at-risk identification (dropout).}}{20}{figure.caption.8}\protected@file@percent }
\newlabel{fig:methodology_objectives}{{2}{20}{\textbf {Research Objectives Breakdown with Dual-Task Analysis.} Hierarchical decomposition of the main research question ``Can deep learning + LLM predict student outcomes and provide actionable interventions?'' into two parallel objectives: Objective 1 (Student Performance Prediction) addresses 3-class classification through 5 systematic sub-tasks from baseline model development (Task 1.1: PPN) through multi-metric evaluation (Task 1.5), achieving 76.4\% accuracy with interpretable attention-based predictions; Objective 2 (Student Dropout Prediction) implements binary classification via 5 sub-tasks including attention-based architecture (Task 2.1: DPN-A), temporal feature engineering, and ROC-AUC optimization, achieving superior 87.05\% accuracy with 0.910 AUC-ROC. The integrated multi-task learning analysis (bottom) reveals task interference phenomenon (67.9\% dropout accuracy in HMTL vs 87.05\% in dedicated DPN-A), validating single-task model superiority for this dataset. The LLM enhancement layer (purple box) demonstrates how GPT-4 integration transforms statistical predictions into personalized, actionable student support recommendations. This dual-objective framework addresses both institutional needs: comprehensive student outcome categorization (performance) and targeted at-risk identification (dropout)}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Feature Selection and Dimensionality Reduction}{22}{subsubsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Data Partitioning Strategy}{23}{subsubsection.4.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Cross-Validation Protocol}{23}{subsubsection.4.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Variables and Operationalization}{24}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Demographic Features}{24}{subsubsection.4.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Academic Features}{24}{subsubsection.4.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Socioeconomic Features}{24}{subsubsection.4.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Macroeconomic Indicators}{24}{subsubsection.4.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.5}Target Variable}{24}{subsubsection.4.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Data Quality and Integrity}{24}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Missing Data Assessment}{24}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Validation Checks}{24}{subsubsection.4.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Ethical Considerations}{24}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Feature Engineering and Preprocessing}{25}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Feature Construction}{25}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Academic Performance Indicators}{25}{subsubsection.5.1.1}\protected@file@percent }
\newlabel{eq:total_units}{{24}{25}{Academic Performance Indicators}{equation.5.24}{}}
\newlabel{eq:approved_units}{{25}{25}{Academic Performance Indicators}{equation.5.25}{}}
\newlabel{eq:success_rate}{{26}{25}{Academic Performance Indicators}{equation.5.26}{}}
\newlabel{eq:consistency}{{27}{25}{Academic Performance Indicators}{equation.5.27}{}}
\newlabel{eq:progression}{{28}{25}{Academic Performance Indicators}{equation.5.28}{}}
\newlabel{eq:avg_grade}{{29}{25}{Academic Performance Indicators}{equation.5.29}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Engagement Metrics}{26}{subsubsection.5.1.2}\protected@file@percent }
\newlabel{eq:no_eval}{{30}{26}{Engagement Metrics}{equation.5.30}{}}
\newlabel{eq:engagement}{{31}{26}{Engagement Metrics}{equation.5.31}{}}
\newlabel{eq:eval_rate}{{32}{26}{Engagement Metrics}{equation.5.32}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Socioeconomic Composite Indicators}{26}{subsubsection.5.1.3}\protected@file@percent }
\newlabel{eq:parent_ed}{{33}{26}{Socioeconomic Composite Indicators}{equation.5.33}{}}
\newlabel{eq:economic}{{34}{26}{Socioeconomic Composite Indicators}{equation.5.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Data Transformation Strategy}{26}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Categorical Encoding}{26}{subsubsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Numerical Normalization}{27}{subsubsection.5.2.2}\protected@file@percent }
\newlabel{eq:zscore}{{35}{27}{Numerical Normalization}{equation.5.35}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Scaling Rationale}{27}{subsubsection.5.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Feature Selection and Dimensionality Reduction}{27}{subsection.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Correlation Analysis}{27}{subsubsection.5.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Variance Threshold}{27}{subsubsection.5.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Feature Importance Ranking}{28}{subsubsection.5.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Data Partitioning Strategy}{28}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Train-Validation-Test Split}{28}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Stratification Rationale}{28}{subsubsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Cross-Validation Protocol}{28}{subsection.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Data Processing and Preprocessing Pipeline (10-Stage Workflow).} Comprehensive visualization of the data transformation pipeline: Stage 1 (Raw Data Input) ingests 4,424 student records from institutional database with 35 base features; Stage 2 (Initial Validation) performs 4 quality checks (missing values detection, duplicate removal, outlier detection via IQR, data type validation); Stage 3 (Feature Engineering) constructs 12 derived features through 6 equations (total units, success rate, semester consistency, academic progression, parental education, economic stability); Stage 4 (Theoretical Mapping) aligns features with Tinto (68\%, 31 features) and Bean (32\%, 15 features) frameworks; Stage 5 (Missing Value Imputation) applies median imputation for numerical features (18) and mode imputation for categorical features (16); Stage 6 (Categorical Encoding) processes binary (direct 0/1), ordinal (label encoding), and nominal (one-hot encoding) variables while creating 3-class target encoding (Graduate=2, Enrolled=1, Dropout=0); Stage 7 (Feature Selection) implements 3-step dimensionality reduction (correlation threshold |r|>0.95, variance threshold <0.01, Random Forest importance ranking retaining 95\% cumulative importance); Stage 8 (Normalization) applies Z-score standardization $(X_{\text  {norm}} = \frac  {X - \mu }{\sigma })$ computed exclusively on training set to prevent data leakage; Stage 9 (Train/Val/Test Split) creates stratified 80/10/10 split preserving class distribution (training: 3,539, validation: 442, test: 443 students); Stage 10 (Tensor Conversion) transforms processed data to PyTorch tensors with shape verification and dtype validation for model compatibility. Color-coded stages: green (input/validation), orange (feature work), purple (theoretical), yellow (imputation/encoding), blue (selection/normalization), red (splitting), teal (output). This systematic pipeline ensures data quality, reproducibility, and prevention of common pitfalls like data leakage and class imbalance.}}{29}{figure.caption.15}\protected@file@percent }
\newlabel{fig:methodology_dataflow}{{3}{29}{\textbf {Data Processing and Preprocessing Pipeline (10-Stage Workflow).} Comprehensive visualization of the data transformation pipeline: Stage 1 (Raw Data Input) ingests 4,424 student records from institutional database with 35 base features; Stage 2 (Initial Validation) performs 4 quality checks (missing values detection, duplicate removal, outlier detection via IQR, data type validation); Stage 3 (Feature Engineering) constructs 12 derived features through 6 equations (total units, success rate, semester consistency, academic progression, parental education, economic stability); Stage 4 (Theoretical Mapping) aligns features with Tinto (68\%, 31 features) and Bean (32\%, 15 features) frameworks; Stage 5 (Missing Value Imputation) applies median imputation for numerical features (18) and mode imputation for categorical features (16); Stage 6 (Categorical Encoding) processes binary (direct 0/1), ordinal (label encoding), and nominal (one-hot encoding) variables while creating 3-class target encoding (Graduate=2, Enrolled=1, Dropout=0); Stage 7 (Feature Selection) implements 3-step dimensionality reduction (correlation threshold |r|>0.95, variance threshold <0.01, Random Forest importance ranking retaining 95\% cumulative importance); Stage 8 (Normalization) applies Z-score standardization $(X_{\text {norm}} = \frac {X - \mu }{\sigma })$ computed exclusively on training set to prevent data leakage; Stage 9 (Train/Val/Test Split) creates stratified 80/10/10 split preserving class distribution (training: 3,539, validation: 442, test: 443 students); Stage 10 (Tensor Conversion) transforms processed data to PyTorch tensors with shape verification and dtype validation for model compatibility. Color-coded stages: green (input/validation), orange (feature work), purple (theoretical), yellow (imputation/encoding), blue (selection/normalization), red (splitting), teal (output). This systematic pipeline ensures data quality, reproducibility, and prevention of common pitfalls like data leakage and class imbalance}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Deep Learning Architectures}{30}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Model 1: Performance Prediction Network (PPN)}{30}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.1}Architecture Design}{30}{subsubsection.7.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.2}Architectural Justification}{30}{subsubsection.7.1.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Performance Prediction Network Forward Pass}}{31}{algorithm.1}\protected@file@percent }
\newlabel{algo:ppn}{{1}{31}{Performance Prediction Network Forward Pass}{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.1.3}Training Configuration}{32}{subsubsection.7.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Model 2: Dropout Prediction Network with Attention (DPN-A)}{32}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1}Architecture and Attention Mechanism}{32}{subsubsection.7.2.1}\protected@file@percent }
\newlabel{eq:attention_energy}{{36}{32}{Architecture and Attention Mechanism}{equation.7.36}{}}
\newlabel{eq:attention_weights}{{37}{32}{Architecture and Attention Mechanism}{equation.7.37}{}}
\newlabel{eq:attention_output}{{38}{32}{Architecture and Attention Mechanism}{equation.7.38}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.2}Attention Mechanism Benefits}{32}{subsubsection.7.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.3}Training Configuration}{33}{subsubsection.7.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Model 3: Hybrid Multi-Task Learning Network (HMTL)}{33}{subsection.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Architecture Design}{33}{subsubsection.7.3.1}\protected@file@percent }
\newlabel{eq:mtl_loss}{{39}{33}{Architecture Design}{equation.7.39}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Hybrid Multi-Task Learning Forward Pass}}{33}{algorithm.2}\protected@file@percent }
\newlabel{algo:hmtl}{{2}{33}{Hybrid Multi-Task Learning Forward Pass}{algorithm.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Multi-Task Learning Rationale}{34}{subsubsection.7.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Baseline Models for Comparative Analysis}{34}{subsection.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}Large Language Model Integration for Personalized Recommendations}{34}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}LLM-Based Recommendation Architecture}{34}{subsection.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.1}System Overview}{34}{subsubsection.8.1.1}\protected@file@percent }
\newlabel{eq:llm_pipeline}{{40}{34}{System Overview}{equation.8.40}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.2}Student Profile Construction}{35}{subsubsection.8.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.3}GPT-4 Configuration}{35}{subsubsection.8.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.1.4}Rule-Based Fallback System}{35}{subsubsection.8.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Recommendation Validation Criteria}{36}{subsection.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Evaluation Metrics and Statistical Testing}{36}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Classification Performance Metrics}{36}{subsection.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.1}Multi-Class Evaluation (PPN \& HMTL)}{36}{subsubsection.9.1.1}\protected@file@percent }
\newlabel{eq:accuracy}{{41}{36}{Multi-Class Evaluation (PPN \& HMTL)}{equation.9.41}{}}
\newlabel{eq:precision_macro}{{42}{36}{Multi-Class Evaluation (PPN \& HMTL)}{equation.9.42}{}}
\newlabel{eq:recall_macro}{{43}{36}{Multi-Class Evaluation (PPN \& HMTL)}{equation.9.43}{}}
\newlabel{eq:f1_macro}{{44}{36}{Multi-Class Evaluation (PPN \& HMTL)}{equation.9.44}{}}
\newlabel{eq:f1_weighted}{{45}{37}{Multi-Class Evaluation (PPN \& HMTL)}{equation.9.45}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.1.2}Binary Classification Evaluation (DPN-A)}{37}{subsubsection.9.1.2}\protected@file@percent }
\newlabel{eq:mcc}{{46}{37}{Binary Classification Evaluation (DPN-A)}{equation.9.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Statistical Significance Testing}{37}{subsection.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.1}McNemar's Test for Pairwise Comparisons}{37}{subsubsection.9.2.1}\protected@file@percent }
\newlabel{eq:mcnemar}{{47}{37}{McNemar's Test for Pairwise Comparisons}{equation.9.47}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.2}Friedman Test with Post-Hoc Nemenyi Correction}{37}{subsubsection.9.2.2}\protected@file@percent }
\newlabel{eq:friedman}{{48}{38}{Friedman Test with Post-Hoc Nemenyi Correction}{equation.9.48}{}}
\newlabel{eq:nemenyi}{{49}{38}{Friedman Test with Post-Hoc Nemenyi Correction}{equation.9.49}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Model Calibration Analysis}{38}{subsection.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.1}Calibration Curves}{38}{subsubsection.9.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.2}Expected Calibration Error (ECE)}{38}{subsubsection.9.3.2}\protected@file@percent }
\newlabel{eq:ece}{{50}{38}{Expected Calibration Error (ECE)}{equation.9.50}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Feature Importance Analysis}{38}{subsection.9.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.4.1}SHAP (SHapley Additive exPlanations)}{38}{subsubsection.9.4.1}\protected@file@percent }
\newlabel{eq:shap}{{51}{38}{SHAP (SHapley Additive exPlanations)}{equation.9.51}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.4.2}Permutation Importance}{39}{subsubsection.9.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Implementation and Computational Resources}{39}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Software Stack}{39}{subsection.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Computational Requirements}{39}{subsection.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Reproducibility Provisions}{39}{subsection.10.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.1}Random Seed Fixation}{39}{subsubsection.10.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.2}Code Availability}{39}{subsubsection.10.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.3}Environment Replication}{39}{subsubsection.10.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Experimental Protocol and Validation Procedure}{40}{section.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Hyperparameter Optimization}{40}{subsection.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2}Training Procedure}{40}{subsection.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3}Cross-Validation and Statistical Robustness}{40}{subsection.11.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4}Test Set Evaluation Workflow}{40}{subsection.11.4}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Model Training and Validation Loop}}{41}{algorithm.3}\protected@file@percent }
\newlabel{algo:training}{{3}{41}{Model Training and Validation Loop}{algorithm.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12}Results and Findings}{42}{section.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1}Baseline Model Performance}{42}{subsection.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.1}Random Forest Classifier}{42}{subsubsection.12.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {26}{\ignorespaces Random Forest Performance (3-Class Performance Prediction)}}{43}{table.caption.29}\protected@file@percent }
\newlabel{tab:rf_results}{{26}{43}{Random Forest Performance (3-Class Performance Prediction)}{table.caption.29}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.1.2}Logistic Regression (Dropout Prediction)}{43}{subsubsection.12.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {27}{\ignorespaces Logistic Regression Performance (Binary Dropout Prediction)}}{44}{table.caption.30}\protected@file@percent }
\newlabel{tab:lr_results}{{27}{44}{Logistic Regression Performance (Binary Dropout Prediction)}{table.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2}Deep Learning Model Performance}{44}{subsection.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.1}Performance Prediction Network (PPN)}{44}{subsubsection.12.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {28}{\ignorespaces PPN Test Set Performance}}{45}{table.caption.31}\protected@file@percent }
\newlabel{tab:ppn_results}{{28}{45}{PPN Test Set Performance}{table.caption.31}{}}
\@writefile{lot}{\contentsline {table}{\numberline {29}{\ignorespaces PPN Class-Wise Performance}}{45}{table.caption.32}\protected@file@percent }
\newlabel{tab:ppn_class}{{29}{45}{PPN Class-Wise Performance}{table.caption.32}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.2}Dropout Prediction with Attention (DPN-A)}{46}{subsubsection.12.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {30}{\ignorespaces DPN-A Test Set Performance}}{47}{table.caption.33}\protected@file@percent }
\newlabel{tab:dpna_results}{{30}{47}{DPN-A Test Set Performance}{table.caption.33}{}}
\@writefile{lot}{\contentsline {table}{\numberline {31}{\ignorespaces Top 10 Input Features by Weight Magnitude}}{48}{table.caption.34}\protected@file@percent }
\newlabel{tab:feature_importance}{{31}{48}{Top 10 Input Features by Weight Magnitude}{table.caption.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.2.3}Hybrid Multi-Task Learning Network (HMTL)}{48}{subsubsection.12.2.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {32}{\ignorespaces HMTL Multi-Task Performance}}{49}{table.caption.35}\protected@file@percent }
\newlabel{tab:hmtl_results}{{32}{49}{HMTL Multi-Task Performance}{table.caption.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3}Model Comparison and Statistical Significance}{49}{subsection.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.4}Visualization Analysis}{50}{subsection.12.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.4.1}Figure 1 \& 10: Integrated Research Objectives Visualization}{50}{subsubsection.12.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.4.2}Figure 2: ROC Curves}{51}{subsubsection.12.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.4.3}Figure 3 \& 4: Confusion Matrices}{52}{subsubsection.12.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.4.4}Figure 3 \& 4: Confusion Matrices}{52}{subsubsection.12.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.4.5}Figure 5: Attention Heatmap}{52}{subsubsection.12.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.4.6}Figure 6: Feature Importance}{53}{subsubsection.12.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.5}Cross-Validation Stability Analysis}{53}{subsection.12.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {38}{\ignorespaces Cross-Validation Results (Mean $\pm $ Std Dev)}}{54}{table.caption.41}\protected@file@percent }
\newlabel{tab:cv_results}{{38}{54}{Cross-Validation Results (Mean $\pm $ Std Dev)}{table.caption.41}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.6}Computational Efficiency}{54}{subsection.12.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {40}{\ignorespaces Training Time and Resource Usage (CPU Infrastructure)}}{54}{table.caption.43}\protected@file@percent }
\newlabel{tab:compute}{{40}{54}{Training Time and Resource Usage (CPU Infrastructure)}{table.caption.43}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.7}Error Analysis}{55}{subsection.12.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.7.1}Common Misclassification Patterns}{55}{subsubsection.12.7.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {41}{\ignorespaces Top Misclassification Patterns (PPN)}}{55}{table.caption.44}\protected@file@percent }
\newlabel{tab:errors}{{41}{55}{Top Misclassification Patterns (PPN)}{table.caption.44}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {12.7.2}Feature Correlation with Errors}{56}{subsubsection.12.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.8}Summary of Key Results}{56}{subsection.12.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13}Limitations and Validity Considerations}{57}{section.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1}Internal Validity Threats}{57}{subsection.13.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.1.1}Confounding Variables}{57}{subsubsection.13.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.1.2}Temporal Effects}{57}{subsubsection.13.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2}External Validity Limitations}{57}{subsection.13.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.2.1}Institutional Specificity}{57}{subsubsection.13.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.2.2}Domain Transfer}{58}{subsubsection.13.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3}Construct Validity Issues}{58}{subsection.13.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.3.1}Outcome Operationalization}{58}{subsubsection.13.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.3.2}Feature Completeness}{58}{subsubsection.13.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4}Statistical Conclusion Validity}{58}{subsection.13.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.4.1}Multiple Comparisons}{58}{subsubsection.13.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {13.4.2}Assumption Verification}{58}{subsubsection.13.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14}Expected Outcomes and Research Impact}{59}{section.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1}Anticipated Findings}{59}{subsection.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.2}Practical Applications}{59}{subsection.14.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.3}Contribution to Knowledge}{60}{subsection.14.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.4}Future Research Directions}{60}{subsection.14.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.4.1}Cross-Institutional Validation and Generalizability}{60}{subsubsection.14.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.4.2}Methodological Extensions}{61}{subsubsection.14.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {14.4.3}Institutional Deployment and Impact Assessment}{62}{subsubsection.14.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {15}Conclusion}{63}{section.15}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Academic performance feature variables with detailed specifications.}}{64}{table.caption.5}\protected@file@percent }
\newlabel{tab:academic_features_detailed}{{5}{64}{Academic performance feature variables with detailed specifications}{table.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Engineered feature variables with descriptive statistics.}}{65}{table.caption.6}\protected@file@percent }
\newlabel{tab:engineered_features}{{6}{65}{Engineered feature variables with descriptive statistics}{table.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Data partition allocation with stratified class distribution.}}{66}{table.caption.9}\protected@file@percent }
\newlabel{tab:data_partition}{{7}{66}{Data partition allocation with stratified class distribution}{table.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Demographic Feature Variables ($n=5$)}}{66}{table.caption.10}\protected@file@percent }
\newlabel{tab:demographic_features}{{8}{66}{Demographic Feature Variables ($n=5$)}{table.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Academic Feature Variables ($n=19$)}}{67}{table.caption.11}\protected@file@percent }
\newlabel{tab:academic_features}{{9}{67}{Academic Feature Variables ($n=19$)}{table.caption.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Socioeconomic Feature Variables ($n=4$)}}{68}{table.caption.12}\protected@file@percent }
\newlabel{tab:socioeconomic_features}{{10}{68}{Socioeconomic Feature Variables ($n=4$)}{table.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Macroeconomic Feature Variables ($n=3$)}}{68}{table.caption.13}\protected@file@percent }
\newlabel{tab:macro_features}{{11}{68}{Macroeconomic Feature Variables ($n=3$)}{table.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces Target Variable Specification}}{69}{table.caption.14}\protected@file@percent }
\newlabel{tab:target_variable}{{12}{69}{Target Variable Specification}{table.caption.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {13}{\ignorespaces Data Partition Allocation}}{69}{table.caption.16}\protected@file@percent }
\newlabel{tab:data_split}{{13}{69}{Data Partition Allocation}{table.caption.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {14}{\ignorespaces Deep Learning Model Architecture Specifications}}{70}{table.caption.17}\protected@file@percent }
\newlabel{tab:model_architectures}{{14}{70}{Deep Learning Model Architecture Specifications}{table.caption.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {15}{\ignorespaces PPN Architectural Specifications}}{71}{table.caption.18}\protected@file@percent }
\newlabel{tab:ppn_arch}{{15}{71}{PPN Architectural Specifications}{table.caption.18}{}}
\@writefile{lot}{\contentsline {table}{\numberline {16}{\ignorespaces PPN Training Hyperparameters}}{71}{table.caption.19}\protected@file@percent }
\newlabel{tab:ppn_training}{{16}{71}{PPN Training Hyperparameters}{table.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {17}{\ignorespaces DPN-A Architectural Specifications}}{72}{table.caption.20}\protected@file@percent }
\newlabel{tab:dpna_arch}{{17}{72}{DPN-A Architectural Specifications}{table.caption.20}{}}
\@writefile{lot}{\contentsline {table}{\numberline {18}{\ignorespaces DPN-A Training Hyperparameters}}{72}{table.caption.21}\protected@file@percent }
\newlabel{tab:dpna_training}{{18}{72}{DPN-A Training Hyperparameters}{table.caption.21}{}}
\@writefile{lot}{\contentsline {table}{\numberline {19}{\ignorespaces Baseline Model Configurations}}{73}{table.caption.22}\protected@file@percent }
\newlabel{tab:baselines}{{19}{73}{Baseline Model Configurations}{table.caption.22}{}}
\@writefile{lot}{\contentsline {table}{\numberline {20}{\ignorespaces GPT-4 Integration Parameters}}{73}{table.caption.23}\protected@file@percent }
\newlabel{tab:gpt4_config}{{20}{73}{GPT-4 Integration Parameters}{table.caption.23}{}}
\@writefile{lot}{\contentsline {table}{\numberline {21}{\ignorespaces GPT-4 Generated Personalized Intervention Recommendations}}{74}{table.caption.24}\protected@file@percent }
\newlabel{tab:llm_recommendations}{{21}{74}{GPT-4 Generated Personalized Intervention Recommendations}{table.caption.24}{}}
\@writefile{lot}{\contentsline {table}{\numberline {22}{\ignorespaces Software and Library Specifications}}{75}{table.caption.25}\protected@file@percent }
\newlabel{tab:software}{{22}{75}{Software and Library Specifications}{table.caption.25}{}}
\@writefile{lot}{\contentsline {table}{\numberline {23}{\ignorespaces Hardware Configuration and Runtime Estimates}}{76}{table.caption.26}\protected@file@percent }
\newlabel{tab:hardware}{{23}{76}{Hardware Configuration and Runtime Estimates}{table.caption.26}{}}
\@writefile{lot}{\contentsline {table}{\numberline {24}{\ignorespaces Comprehensive Hyperparameter Tuning Results}}{77}{table.caption.27}\protected@file@percent }
\newlabel{tab:hyperparameter_tuning}{{24}{77}{Comprehensive Hyperparameter Tuning Results}{table.caption.27}{}}
\@writefile{lot}{\contentsline {table}{\numberline {25}{\ignorespaces 10-Fold Stratified Cross-Validation Results}}{78}{table.caption.28}\protected@file@percent }
\newlabel{tab:cross_validation}{{25}{78}{10-Fold Stratified Cross-Validation Results}{table.caption.28}{}}
\@writefile{lot}{\contentsline {table}{\numberline {33}{\ignorespaces Performance Prediction: PPN vs. Baseline Models (3-Class Classification)}}{79}{table.caption.36}\protected@file@percent }
\newlabel{tab:performance_comparison}{{33}{79}{Performance Prediction: PPN vs. Baseline Models (3-Class Classification)}{table.caption.36}{}}
\@writefile{lot}{\contentsline {table}{\numberline {34}{\ignorespaces Dropout Prediction: DPN-A vs. Baseline Models (Binary Classification)}}{80}{table.caption.37}\protected@file@percent }
\newlabel{tab:dropout_comparison}{{34}{80}{Dropout Prediction: DPN-A vs. Baseline Models (Binary Classification)}{table.caption.37}{}}
\@writefile{lot}{\contentsline {table}{\numberline {35}{\ignorespaces PPN Confusion Matrix (3-Class Performance Prediction)}}{81}{table.caption.38}\protected@file@percent }
\newlabel{tab:ppn_confusion}{{35}{81}{PPN Confusion Matrix (3-Class Performance Prediction)}{table.caption.38}{}}
\@writefile{lot}{\contentsline {table}{\numberline {36}{\ignorespaces DPN-A Confusion Matrix (Binary Dropout Prediction)}}{81}{table.caption.39}\protected@file@percent }
\newlabel{tab:dpna_confusion}{{36}{81}{DPN-A Confusion Matrix (Binary Dropout Prediction)}{table.caption.39}{}}
\@writefile{lot}{\contentsline {table}{\numberline {37}{\ignorespaces DPN-A Attention Weights: Top 15 Features by Importance}}{82}{table.caption.40}\protected@file@percent }
\newlabel{tab:attention_weights}{{37}{82}{DPN-A Attention Weights: Top 15 Features by Importance}{table.caption.40}{}}
\@writefile{lot}{\contentsline {table}{\numberline {39}{\ignorespaces Computational Performance and Resource Requirements}}{83}{table.caption.42}\protected@file@percent }
\newlabel{tab:computational_performance}{{39}{83}{Computational Performance and Resource Requirements}{table.caption.42}{}}
\bibdata{references}
\bibcite{Adnan2021}{{1}{2021}{{Adnan et~al.}}{{Adnan, Habib, Ashraf, Mussadiq, Raza, Abid, Bashir and Khan}}}
\bibcite{Asif2017}{{2}{2017}{{Asif et~al.}}{{Asif, Merceron, Ali and Haider}}}
\bibcite{Bean1985}{{3}{1985}{{Bean}}{{}}}
\bibcite{Chen2020}{{4}{2020}{{Chen et~al.}}{{Chen, Lu, Zheng, Chen and Yang}}}
\bibcite{Huang2020}{{5}{2020}{{Huang et~al.}}{{Huang, Lu and Yang}}}
\bibcite{Kotsiantis2013}{{6}{2013}{{Kotsiantis et~al.}}{{Kotsiantis, Pierrakeas and Pintelas}}}
\bibcite{Liu2019}{{7}{2019}{{Liu et~al.}}{{Liu, Huang, Yin, Chen, Xiong, Su and Hu}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Comprehensive Model Performance Comparison Across Three Metrics.} Three-panel bar chart comparing six models (Random Forest, Logistic Regression, PPN, DPN-A, HMTL-Performance, HMTL-Dropout) across (A) Accuracy, (B) F1-Macro Score, and (C) AUC-ROC. DPN-A achieves highest accuracy (87.05\%) and competitive AUC-ROC (0.910), marginally exceeding baseline Logistic Regression. HMTL dropout task underperforms (67.9\% accuracy), indicating task interference. Error bars represent 95\% confidence intervals from 10-fold cross-validation. Color coding distinguishes baseline models (blue tones) from deep learning models (orange/green tones).}}{85}{figure.caption.45}\protected@file@percent }
\newlabel{fig:model_comparison}{{4}{85}{\textbf {Comprehensive Model Performance Comparison Across Three Metrics.} Three-panel bar chart comparing six models (Random Forest, Logistic Regression, PPN, DPN-A, HMTL-Performance, HMTL-Dropout) across (A) Accuracy, (B) F1-Macro Score, and (C) AUC-ROC. DPN-A achieves highest accuracy (87.05\%) and competitive AUC-ROC (0.910), marginally exceeding baseline Logistic Regression. HMTL dropout task underperforms (67.9\% accuracy), indicating task interference. Error bars represent 95\% confidence intervals from 10-fold cross-validation. Color coding distinguishes baseline models (blue tones) from deep learning models (orange/green tones)}{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {ROC Curves for Dropout Prediction Models.} Receiver Operating Characteristic curves comparing discriminative ability of dropout prediction models: Logistic Regression (AUC=0.920, dashed blue), DPN-A (AUC=0.910, solid orange), and HMTL (AUC=0.843, dotted green). All models significantly outperform random classifier (diagonal gray line, AUC=0.50). DPN-A and Logistic Regression curves closely overlap, indicating comparable true positive rate vs. false positive rate trade-offs. Shaded regions represent 95\% confidence intervals from bootstrap resampling (n=1000). Operating points marked with circles indicate selected classification thresholds (0.5 for all models).}}{86}{figure.caption.46}\protected@file@percent }
\newlabel{fig:roc_curves}{{5}{86}{\textbf {ROC Curves for Dropout Prediction Models.} Receiver Operating Characteristic curves comparing discriminative ability of dropout prediction models: Logistic Regression (AUC=0.920, dashed blue), DPN-A (AUC=0.910, solid orange), and HMTL (AUC=0.843, dotted green). All models significantly outperform random classifier (diagonal gray line, AUC=0.50). DPN-A and Logistic Regression curves closely overlap, indicating comparable true positive rate vs. false positive rate trade-offs. Shaded regions represent 95\% confidence intervals from bootstrap resampling (n=1000). Operating points marked with circles indicate selected classification thresholds (0.5 for all models)}{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {PPN Confusion Matrix for 3-Class Performance Prediction.} Normalized confusion matrix (row percentages) showing PPN classification results on test set (N=664). Diagonal elements represent correct predictions: Dropout (73.7\%), Enrolled (39.5\%), Graduate (91.3\%). "Enrolled" class exhibits highest misclassification rate (60.5\%), primarily confused with Graduate (35.3\%) and Dropout (25.2\%). Graduate class achieves best recall (91.3\%), rarely confused with Dropout (1.5\%). Raw counts displayed in cells. Color intensity indicates classification frequency (dark blue = high, light yellow = low). Overall accuracy: 76.4\%.}}{87}{figure.caption.47}\protected@file@percent }
\newlabel{fig:ppn_cm}{{6}{87}{\textbf {PPN Confusion Matrix for 3-Class Performance Prediction.} Normalized confusion matrix (row percentages) showing PPN classification results on test set (N=664). Diagonal elements represent correct predictions: Dropout (73.7\%), Enrolled (39.5\%), Graduate (91.3\%). "Enrolled" class exhibits highest misclassification rate (60.5\%), primarily confused with Graduate (35.3\%) and Dropout (25.2\%). Graduate class achieves best recall (91.3\%), rarely confused with Dropout (1.5\%). Raw counts displayed in cells. Color intensity indicates classification frequency (dark blue = high, light yellow = low). Overall accuracy: 76.4\%}{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {DPN-A Confusion Matrix for Binary Dropout Prediction.} Normalized confusion matrix showing DPN-A classification performance: True Negatives (94.0\%, n=424), True Positives (72.3\%, n=154), False Positives (6.0\%, n=27), False Negatives (27.7\%, n=59). High specificity (94.0\%) indicates strong ability to correctly identify non-dropout students, minimizing false alarms for intervention programs. Moderate sensitivity (72.3\%) reflects trade-off prioritizing precision (85.1\%) over recall. Overall accuracy: 87.05\%, AUC-ROC: 0.910. Raw counts displayed in cells.}}{88}{figure.caption.48}\protected@file@percent }
\newlabel{fig:dpna_cm}{{7}{88}{\textbf {DPN-A Confusion Matrix for Binary Dropout Prediction.} Normalized confusion matrix showing DPN-A classification performance: True Negatives (94.0\%, n=424), True Positives (72.3\%, n=154), False Positives (6.0\%, n=27), False Negatives (27.7\%, n=59). High specificity (94.0\%) indicates strong ability to correctly identify non-dropout students, minimizing false alarms for intervention programs. Moderate sensitivity (72.3\%) reflects trade-off prioritizing precision (85.1\%) over recall. Overall accuracy: 87.05\%, AUC-ROC: 0.910. Raw counts displayed in cells}{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textbf  {Attention Weight Heatmap Stratified by Dropout Risk.} Self-attention weights from DPN-A hidden layer (15 dimensions $\times $ 20 students) stratified by predicted dropout probability: High-risk (P$>$0.7, n=7, top rows), Medium-risk (0.3$\leq $P$\leq $0.7, n=7, middle rows), Low-risk (P$<$0.3, n=6, bottom rows). High-risk students exhibit concentrated activation in specific dimensions (dims 3, 7, 12), suggesting learned risk signatures. Low-risk students show diffuse attention patterns (uniform yellow-green coloring). Medium-risk students display intermediate heterogeneity. Color bar indicates attention magnitude (0.0--0.4 scale). Demonstrates model's ability to learn personalized risk representations for individualized interventions.}}{89}{figure.caption.49}\protected@file@percent }
\newlabel{fig:attention_heatmap}{{8}{89}{\textbf {Attention Weight Heatmap Stratified by Dropout Risk.} Self-attention weights from DPN-A hidden layer (15 dimensions $\times $ 20 students) stratified by predicted dropout probability: High-risk (P$>$0.7, n=7, top rows), Medium-risk (0.3$\leq $P$\leq $0.7, n=7, middle rows), Low-risk (P$<$0.3, n=6, bottom rows). High-risk students exhibit concentrated activation in specific dimensions (dims 3, 7, 12), suggesting learned risk signatures. Low-risk students show diffuse attention patterns (uniform yellow-green coloring). Medium-risk students display intermediate heterogeneity. Color bar indicates attention magnitude (0.0--0.4 scale). Demonstrates model's ability to learn personalized risk representations for individualized interventions}{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \textbf  {Top 20 Features by Input Layer Weight Magnitude with Theoretical Alignment.} Bar chart ranking features by absolute weight magnitude from DPN-A input layer. Top 5 features all academic (Tinto factors, orange bars): curricular units 2nd semester grade (0.342), curricular units 1st semester grade (0.318), success rate (0.276), average grade (0.264), academic progression (0.142). Financial/environmental factors (Bean factors, blue bars) rank 5th--7th: tuition fees up-to-date (0.189), scholarship holder (0.171), parental education (0.158). Cumulative importance: Tinto 68\%, Bean 32\%. Validates integrated theoretical framework and guides intervention priorities (academic support primary, financial aid supplementary).}}{90}{figure.caption.50}\protected@file@percent }
\newlabel{fig:feature_importance}{{9}{90}{\textbf {Top 20 Features by Input Layer Weight Magnitude with Theoretical Alignment.} Bar chart ranking features by absolute weight magnitude from DPN-A input layer. Top 5 features all academic (Tinto factors, orange bars): curricular units 2nd semester grade (0.342), curricular units 1st semester grade (0.318), success rate (0.276), average grade (0.264), academic progression (0.142). Financial/environmental factors (Bean factors, blue bars) rank 5th--7th: tuition fees up-to-date (0.189), scholarship holder (0.171), parental education (0.158). Cumulative importance: Tinto 68\%, Bean 32\%. Validates integrated theoretical framework and guides intervention priorities (academic support primary, financial aid supplementary)}{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \textbf  {Training and Validation Loss Curves for PPN and DPN-A.} Two-panel plot showing loss convergence over training epochs: (A) PPN 3-class performance prediction (32 epochs, early stopping), (B) DPN-A binary dropout prediction (29 epochs, early stopping). Training loss (solid lines) decreases monotonically without oscillation. Validation loss (dashed lines) plateaus with slight divergence in final epochs, triggering early stopping. No evidence of overfitting (validation loss remains within 10\% of training loss). PPN final losses: Train=0.4885, Val=0.5365. DPN-A final losses: Train=0.2517, Val=0.2983. Shaded regions indicate standard deviation across 5 random seeds.}}{91}{figure.caption.51}\protected@file@percent }
\newlabel{fig:training_curves}{{10}{91}{\textbf {Training and Validation Loss Curves for PPN and DPN-A.} Two-panel plot showing loss convergence over training epochs: (A) PPN 3-class performance prediction (32 epochs, early stopping), (B) DPN-A binary dropout prediction (29 epochs, early stopping). Training loss (solid lines) decreases monotonically without oscillation. Validation loss (dashed lines) plateaus with slight divergence in final epochs, triggering early stopping. No evidence of overfitting (validation loss remains within 10\% of training loss). PPN final losses: Train=0.4885, Val=0.5365. DPN-A final losses: Train=0.2517, Val=0.2983. Shaded regions indicate standard deviation across 5 random seeds}{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \textbf  {Class Distribution in Educational Dataset.} Two-panel visualization of outcome distribution: (A) Pie chart showing proportions (Graduate 49.9\%, Dropout 32.1\%, Enrolled 17.9\%), (B) Bar chart with counts (Graduate n=2,209, Dropout n=1,421, Enrolled n=794). Moderate imbalance addressed via class-weighted loss functions in training. "Enrolled" minority class (17.9\%) presents modeling challenge reflected in lower recall across all models (39.5--42\%). Dataset total: N=4,424 students. Color scheme consistent across all figures (Graduate=blue, Dropout=orange, Enrolled=green).}}{92}{figure.caption.52}\protected@file@percent }
\newlabel{fig:class_distribution}{{11}{92}{\textbf {Class Distribution in Educational Dataset.} Two-panel visualization of outcome distribution: (A) Pie chart showing proportions (Graduate 49.9\%, Dropout 32.1\%, Enrolled 17.9\%), (B) Bar chart with counts (Graduate n=2,209, Dropout n=1,421, Enrolled n=794). Moderate imbalance addressed via class-weighted loss functions in training. "Enrolled" minority class (17.9\%) presents modeling challenge reflected in lower recall across all models (39.5--42\%). Dataset total: N=4,424 students. Color scheme consistent across all figures (Graduate=blue, Dropout=orange, Enrolled=green)}{figure.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \textbf  {Precision-Recall Curves for Dropout Prediction (Supplementary).} Precision-Recall curves for imbalanced dropout class: DPN-A (AUC-PR=0.878, solid orange), Logistic Regression (AUC-PR=0.863, dashed blue), HMTL (AUC-PR=0.741, dotted green). DPN-A achieves highest precision at fixed recall levels, indicating superior performance on minority dropout class. Baseline precision (horizontal dashed line at 0.321) represents dropout prevalence in test set. All models significantly exceed baseline. Knee points indicate optimal precision-recall trade-offs: DPN-A operates at (Recall=0.72, Precision=0.85). Shaded regions represent 95\% confidence intervals from stratified bootstrap (n=1000).}}{93}{figure.caption.53}\protected@file@percent }
\newlabel{fig:pr_curves}{{12}{93}{\textbf {Precision-Recall Curves for Dropout Prediction (Supplementary).} Precision-Recall curves for imbalanced dropout class: DPN-A (AUC-PR=0.878, solid orange), Logistic Regression (AUC-PR=0.863, dashed blue), HMTL (AUC-PR=0.741, dotted green). DPN-A achieves highest precision at fixed recall levels, indicating superior performance on minority dropout class. Baseline precision (horizontal dashed line at 0.321) represents dropout prevalence in test set. All models significantly exceed baseline. Knee points indicate optimal precision-recall trade-offs: DPN-A operates at (Recall=0.72, Precision=0.85). Shaded regions represent 95\% confidence intervals from stratified bootstrap (n=1000)}{figure.caption.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces \textbf  {Integrated Dual-Task Research Analysis: Performance Prediction vs Dropout Prediction.} Four-panel comparative visualization demonstrating both research objectives: (A) Performance Prediction Task (3-class) showing PPN confusion matrix with Graduate class achieving 91.3\% recall, Enrolled class struggling at 39.5\%, and Dropout at 73.7\%; (B) Dropout Prediction Task (binary) showing DPN-A confusion matrix with 94.0\% specificity and 72.3\% sensitivity; (C) Class-wise F1-score comparison revealing PPN's balanced performance across three classes (F1=0.762, 0.439, 0.863) versus DPN-A's focused binary dropout detection (F1=0.782); (D) Overall task complexity analysis comparing 3-class performance prediction (76.4\% accuracy, F1-Macro=0.688) against binary dropout prediction (87.05\% accuracy, F1-Macro=0.782). This integrated view validates the research design addressing both institutional objectives: comprehensive student outcome categorization (Performance task) and targeted at-risk identification (Dropout task). Color scheme: Blue/green for performance task, orange/red for dropout task.}}{94}{figure.caption.54}\protected@file@percent }
\newlabel{fig:dual_task_comparison}{{13}{94}{\textbf {Integrated Dual-Task Research Analysis: Performance Prediction vs Dropout Prediction.} Four-panel comparative visualization demonstrating both research objectives: (A) Performance Prediction Task (3-class) showing PPN confusion matrix with Graduate class achieving 91.3\% recall, Enrolled class struggling at 39.5\%, and Dropout at 73.7\%; (B) Dropout Prediction Task (binary) showing DPN-A confusion matrix with 94.0\% specificity and 72.3\% sensitivity; (C) Class-wise F1-score comparison revealing PPN's balanced performance across three classes (F1=0.762, 0.439, 0.863) versus DPN-A's focused binary dropout detection (F1=0.782); (D) Overall task complexity analysis comparing 3-class performance prediction (76.4\% accuracy, F1-Macro=0.688) against binary dropout prediction (87.05\% accuracy, F1-Macro=0.782). This integrated view validates the research design addressing both institutional objectives: comprehensive student outcome categorization (Performance task) and targeted at-risk identification (Dropout task). Color scheme: Blue/green for performance task, orange/red for dropout task}{figure.caption.54}{}}
\bibcite{Martinez2023}{{8}{2023}{{Martinez et~al.}}{{Martinez, Rodriguez and Garcia}}}
\bibcite{Nguyen2024}{{9}{2024}{{Nguyen et~al.}}{{Nguyen, Chen and Brown}}}
\bibcite{OpenAI2023}{{10}{2023}{{OpenAI}}{{}}}
\bibcite{Romero2020}{{11}{2020}{{Romero and Ventura}}{{}}}
\bibcite{Ruder2017}{{12}{2017}{{Ruder}}{{}}}
\bibcite{Tinto1993}{{13}{1993}{{Tinto}}{{}}}
\bibcite{Vaswani2017}{{14}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser and Polosukhin}}}
\bibcite{Wang2022}{{15}{2022}{{Wang et~al.}}{{Wang, Yu and Miao}}}
\bibcite{Yang2021}{{16}{2021}{{Yang and Brinton}}{{}}}
\@writefile{toc}{\let\numberline\tmptocnumberline}
\@writefile{toc}{\contentsline {section}{\numberline {Appendix~A}Pseudocode for Data Preprocessing}{96}{appendix.A}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Feature Engineering and Normalization Pipeline}}{96}{algorithm.4}\protected@file@percent }
\newlabel{algo:preprocessing}{{4}{96}{Feature Engineering and Normalization Pipeline}{algorithm.4}{}}
\gdef \@abspage@last{96}
