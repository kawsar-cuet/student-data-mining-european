{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c4cd6e2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f986464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.data_preprocessing import DataPreprocessor\n",
    "from src.models.performance_model import PerformanceModel\n",
    "from src.models.dropout_model import DropoutModel\n",
    "from src.llm.recommendation_engine import RecommendationEngine\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "print(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0e9693",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Real Dataset\n",
    "\n",
    "We use the real educational dataset with **4,424 students** and **35 features** including demographic, academic, socioeconomic, and macroeconomic indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6ebacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real educational data\n",
    "data_path = '../data/educational_data.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['Target'].value_counts())\n",
    "print(f\"\\nTarget percentages:\")\n",
    "print(df['Target'].value_counts(normalize=True) * 100)\n",
    "\n",
    "print(f\"\\nColumn names ({len(df.columns)} features):\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59af1981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4de923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Target distribution (Graduate/Dropout/Enrolled)\n",
    "plt.figure(figsize=(10, 6))\n",
    "target_counts = df['Target'].value_counts()\n",
    "colors = ['#2ecc71', '#e74c3c', '#3498db']  # Green, Red, Blue\n",
    "target_counts.plot(kind='bar', color=colors, edgecolor='black', linewidth=1.2)\n",
    "plt.title('Student Outcome Distribution (N=4,424)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Outcome Status', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add counts on bars\n",
    "for i, v in enumerate(target_counts):\n",
    "    plt.text(i, v + 50, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTarget Distribution:\")\n",
    "for target, count in target_counts.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  {target}: {count} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cbc5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary dropout analysis (Graduate+Enrolled vs Dropout)\n",
    "df['is_dropout'] = (df['Target'] == 'Dropout').astype(int)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Subplot 1: Pie chart\n",
    "plt.subplot(1, 2, 1)\n",
    "dropout_counts = df['is_dropout'].value_counts()\n",
    "plt.pie(dropout_counts, labels=['Not Dropout', 'Dropout'], autopct='%1.1f%%', \n",
    "        colors=['lightgreen', 'coral'], startangle=90, explode=(0.05, 0))\n",
    "plt.title('Binary Dropout Status', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Subplot 2: Gender vs Dropout\n",
    "plt.subplot(1, 2, 2)\n",
    "gender_dropout = pd.crosstab(df['Gender'], df['is_dropout'], normalize='index') * 100\n",
    "gender_dropout.plot(kind='bar', color=['lightgreen', 'coral'], edgecolor='black')\n",
    "plt.title('Dropout Rate by Gender', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Gender (0=Female, 1=Male)')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(['Not Dropout', 'Dropout'], loc='upper right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDropout Statistics:\")\n",
    "print(f\"  Total Dropouts: {dropout_counts[1]} ({dropout_counts[1]/len(df)*100:.1f}%)\")\n",
    "print(f\"  Not Dropout: {dropout_counts[0]} ({dropout_counts[0]/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d22df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for key numerical features\n",
    "numerical_features = [\n",
    "    'Age at enrollment',\n",
    "    'Curricular units 1st sem (grade)',\n",
    "    'Curricular units 2nd sem (grade)',\n",
    "    'Curricular units 1st sem (approved)',\n",
    "    'Curricular units 2nd sem (approved)',\n",
    "    'Unemployment rate',\n",
    "    'Inflation rate',\n",
    "    'GDP'\n",
    "]\n",
    "\n",
    "# Select available features\n",
    "available_features = [f for f in numerical_features if f in df.columns]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "corr_matrix = df[available_features].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Correlation Matrix of Key Numerical Features', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf7e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Socioeconomic and demographic analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Age distribution\n",
    "axes[0, 0].hist(df['Age at enrollment'], bins=30, color='purple', \n",
    "                edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_title('Age at Enrollment Distribution', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Age')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Scholarship status\n",
    "scholarship_counts = df['Scholarship holder'].value_counts()\n",
    "axes[0, 1].bar(['No Scholarship', 'Scholarship'], scholarship_counts.values, \n",
    "               color=['lightcoral', 'lightgreen'], edgecolor='black')\n",
    "axes[0, 1].set_title('Scholarship Distribution', fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Marital status\n",
    "marital_counts = df['Marital status'].value_counts().head(5)\n",
    "axes[1, 0].barh(range(len(marital_counts)), marital_counts.values, color='teal', edgecolor='black')\n",
    "axes[1, 0].set_yticks(range(len(marital_counts)))\n",
    "axes[1, 0].set_yticklabels([f\"Status {idx}\" for idx in marital_counts.index])\n",
    "axes[1, 0].set_title('Top 5 Marital Status Categories', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Count')\n",
    "axes[1, 0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Daytime vs Evening attendance\n",
    "attendance_counts = df['Daytime/evening attendance\\t'].value_counts()\n",
    "axes[1, 1].pie(attendance_counts, labels=['Evening', 'Daytime'], autopct='%1.1f%%',\n",
    "               colors=['#f39c12', '#16a085'], startangle=90)\n",
    "axes[1, 1].set_title('Attendance Type', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96944fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Academic performance analysis - First semester grades\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Subplot 1: Grade distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df['Curricular units 1st sem (grade)'], bins=30, color='steelblue', \n",
    "         edgecolor='black', alpha=0.7)\n",
    "plt.axvline(df['Curricular units 1st sem (grade)'].mean(), color='red', \n",
    "            linestyle='--', linewidth=2, label=f\"Mean: {df['Curricular units 1st sem (grade)'].mean():.2f}\")\n",
    "plt.title('1st Semester Grade Distribution', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Grade (0-20 scale)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Subplot 2: Approval rate\n",
    "plt.subplot(1, 2, 2)\n",
    "df['approval_rate_1st'] = df['Curricular units 1st sem (approved)'] / df['Curricular units 1st sem (enrolled)'].replace(0, np.nan)\n",
    "plt.hist(df['approval_rate_1st'].dropna(), bins=30, color='green', \n",
    "         edgecolor='black', alpha=0.7)\n",
    "plt.axvline(df['approval_rate_1st'].mean(), color='red', \n",
    "            linestyle='--', linewidth=2, label=f\"Mean: {df['approval_rate_1st'].mean():.2%}\")\n",
    "plt.title('1st Semester Approval Rate', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Approval Rate')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3158d4",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "Following the journal methodology:\n",
    "1. **Feature Engineering**: Create derived features (success rate, semester consistency, etc.)\n",
    "2. **Encoding**: Handle categorical variables (one-hot, label encoding)\n",
    "3. **Normalization**: Z-score standardization for numerical features\n",
    "4. **Stratified Split**: 70% train, 15% validation, 15% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc4bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual preprocessing for real dataset (until DataPreprocessor is updated)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Create binary dropout target\n",
    "y_dropout = (df['Target'] == 'Dropout').astype(int)\n",
    "\n",
    "# Create 3-class target for performance prediction\n",
    "le_target = LabelEncoder()\n",
    "y_target = le_target.fit_transform(df['Target'])  # Dropout=0, Enrolled=1, Graduate=2\n",
    "\n",
    "print(f\"Target encoding: {dict(zip(le_target.classes_, le_target.transform(le_target.classes_)))}\")\n",
    "\n",
    "# Select numerical features for initial model\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_cols = [col for col in numerical_cols if col not in ['Target', 'is_dropout']]\n",
    "\n",
    "print(f\"\\nUsing {len(numerical_cols)} numerical features\")\n",
    "\n",
    "X = df[numerical_cols].fillna(0)  # Handle any potential NaN values\n",
    "\n",
    "# Stratified train-val-test split (70-15-15)\n",
    "X_temp, X_test, y_target_temp, y_target_test, y_dropout_temp, y_dropout_test = train_test_split(\n",
    "    X, y_target, y_dropout, test_size=0.15, random_state=42, stratify=y_target\n",
    ")\n",
    "\n",
    "X_train, X_val, y_target_train, y_target_val, y_dropout_train, y_dropout_val = train_test_split(\n",
    "    X_temp, y_target_temp, y_dropout_temp, test_size=0.1765, random_state=42, stratify=y_target_temp\n",
    ")  # 0.1765 of 85% â‰ˆ 15% of total\n",
    "\n",
    "# Standardize features (Z-score normalization)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nâœ“ Data preprocessing complete!\")\n",
    "print(f\"Number of features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"Training samples: {len(X_train_scaled)} ({len(X_train_scaled)/len(df)*100:.1f}%)\")\n",
    "print(f\"Validation samples: {len(X_val_scaled)} ({len(X_val_scaled)/len(df)*100:.1f}%)\")\n",
    "print(f\"Test samples: {len(X_test_scaled)} ({len(X_test_scaled)/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTarget distribution in training set:\")\n",
    "print(pd.Series(y_target_train).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caca67ce",
   "metadata": {},
   "source": [
    "## 3. Train Deep Learning Models\n",
    "\n",
    "Following journal methodology architectures:\n",
    "\n",
    "### 3.1 Performance Prediction Network (PPN)\n",
    "Multi-class classification: **Graduate (2)** vs **Enrolled (1)** vs **Dropout (0)**\n",
    "- Architecture: 128 â†’ 64 â†’ 32 neurons\n",
    "- Regularization: Batch Normalization + Dropout (0.3, 0.2, 0.1)\n",
    "- Output: 3-class Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e73aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Performance Prediction Network (PPN) - 3-class classification\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_ppn(input_dim, num_classes=3):\n",
    "    \"\"\"Performance Prediction Network following journal methodology\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        \n",
    "        # Hidden Layer 1: 128 units\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Hidden Layer 2: 64 units\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Hidden Layer 3: 32 units\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.1),\n",
    "        \n",
    "        # Output layer: 3 classes (Dropout, Enrolled, Graduate)\n",
    "        layers.Dense(num_classes, activation='softmax', name='output')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize model\n",
    "ppn_model = build_ppn(input_dim=X_train_scaled.shape[1], num_classes=3)\n",
    "\n",
    "# Compile with categorical cross-entropy\n",
    "ppn_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"âœ“ PPN Model Architecture:\")\n",
    "ppn_model.summary()\n",
    "\n",
    "# Train with early stopping and learning rate reduction\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, restore_best_weights=True, verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7, verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nðŸš€ Training PPN Model...\")\n",
    "ppn_history = ppn_model.fit(\n",
    "    X_train_scaled, y_target_train,\n",
    "    validation_data=(X_val_scaled, y_target_val),\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Training complete! Best epoch: {len(ppn_history.history['loss']) - 20}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de17527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Model Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Model Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0a33ac",
   "metadata": {},
   "source": [
    "### 3.2 Dropout Prediction Network with Attention (DPN-A)\n",
    "Binary classification with **self-attention mechanism** for interpretability\n",
    "- Architecture: 64 â†’ Attention Layer â†’ 32 â†’ 16 neurons\n",
    "- Custom attention layer learns feature importance\n",
    "- Output: Binary Sigmoid (Dropout probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3869da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Dropout Prediction Network with Attention (DPN-A)\n",
    "class AttentionLayer(layers.Layer):\n",
    "    \"\"\"Self-attention layer for feature importance learning\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            name='attention_weight',\n",
    "            shape=(input_shape[-1], input_shape[-1]),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name='attention_bias',\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        # Attention mechanism: Î± = softmax(tanh(xW + b))\n",
    "        e = keras.activations.tanh(keras.backend.dot(x, self.W) + self.b)\n",
    "        alpha = keras.activations.softmax(e)\n",
    "        output = x * alpha\n",
    "        return output\n",
    "\n",
    "def build_dpn_attention(input_dim):\n",
    "    \"\"\"Dropout Prediction Network with Attention following journal methodology\"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        \n",
    "        # Hidden Layer 1: 64 units\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        # Attention Layer\n",
    "        AttentionLayer(),\n",
    "        \n",
    "        # Hidden Layer 2: 32 units\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        # Hidden Layer 3: 16 units\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        \n",
    "        # Output layer: Binary classification\n",
    "        layers.Dense(1, activation='sigmoid', name='dropout_output')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize model\n",
    "dpn_model = build_dpn_attention(input_dim=X_train_scaled.shape[1])\n",
    "\n",
    "# Calculate class weights for imbalanced data\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights_array = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.unique(y_dropout_train), \n",
    "    y=y_dropout_train\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights_array))\n",
    "print(f\"Class weights for imbalanced data: {class_weights}\")\n",
    "\n",
    "# Compile with binary cross-entropy\n",
    "dpn_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ DPN-A Model Architecture:\")\n",
    "dpn_model.summary()\n",
    "\n",
    "print(\"\\nðŸš€ Training DPN-A Model with Attention...\")\n",
    "dpn_history = dpn_model.fit(\n",
    "    X_train_scaled, y_dropout_train,\n",
    "    validation_data=(X_val_scaled, y_dropout_val),\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96ff3c3",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation\n",
    "\n",
    "Comprehensive evaluation following journal methodology:\n",
    "- **Classification Metrics**: Accuracy, Precision, Recall, F1-Score (Macro & Weighted)\n",
    "- **Probabilistic Metrics**: AUC-ROC, AUC-PR\n",
    "- **Confusion Matrices**: Detailed error analysis\n",
    "- **ROC Curves**: Threshold-independent performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef618b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_target_pred_proba = ppn_model.predict(X_test_scaled, verbose=0)\n",
    "y_target_pred = np.argmax(y_target_pred_proba, axis=1)\n",
    "\n",
    "y_dropout_pred_proba = dpn_model.predict(X_test_scaled, verbose=0).flatten()\n",
    "y_dropout_pred = (y_dropout_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Map predictions to labels\n",
    "target_labels = ['Dropout', 'Enrolled', 'Graduate']\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'True_Outcome': [target_labels[i] for i in y_target_test],\n",
    "    'Predicted_Outcome': [target_labels[i] for i in y_target_pred],\n",
    "    'Graduate_Prob': y_target_pred_proba[:, 2],\n",
    "    'Enrolled_Prob': y_target_pred_proba[:, 1],\n",
    "    'Dropout_Prob_PPN': y_target_pred_proba[:, 0],\n",
    "    'Dropout_Prob_DPN': y_dropout_pred_proba,\n",
    "    'True_Dropout_Binary': y_dropout_test,\n",
    "    'Predicted_Dropout_Binary': y_dropout_pred\n",
    "})\n",
    "\n",
    "print(\"âœ“ Predictions complete!\")\n",
    "print(f\"\\nPrediction Results (first 10 students):\")\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation metrics\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
    "                             roc_auc_score, roc_curve, precision_recall_curve, \n",
    "                             average_precision_score, f1_score)\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\" PERFORMANCE PREDICTION NETWORK (PPN) - 3-Class Classification\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "ppn_accuracy = accuracy_score(y_target_test, y_target_pred)\n",
    "ppn_f1_macro = f1_score(y_target_test, y_target_pred, average='macro')\n",
    "ppn_f1_weighted = f1_score(y_target_test, y_target_pred, average='weighted')\n",
    "\n",
    "print(f\"\\nðŸ“Š Overall Metrics:\")\n",
    "print(f\"  Accuracy: {ppn_accuracy:.4f}\")\n",
    "print(f\"  F1-Score (Macro): {ppn_f1_macro:.4f}\")\n",
    "print(f\"  F1-Score (Weighted): {ppn_f1_weighted:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Classification Report:\")\n",
    "print(classification_report(y_target_test, y_target_pred, \n",
    "                           target_names=target_labels, \n",
    "                           digits=4, zero_division=0))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\" DROPOUT PREDICTION NETWORK WITH ATTENTION (DPN-A) - Binary Classification\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "dpn_accuracy = accuracy_score(y_dropout_test, y_dropout_pred)\n",
    "dpn_f1 = f1_score(y_dropout_test, y_dropout_pred)\n",
    "dpn_auc_roc = roc_auc_score(y_dropout_test, y_dropout_pred_proba)\n",
    "dpn_auc_pr = average_precision_score(y_dropout_test, y_dropout_pred_proba)\n",
    "\n",
    "print(f\"\\nðŸ“Š Overall Metrics:\")\n",
    "print(f\"  Accuracy: {dpn_accuracy:.4f}\")\n",
    "print(f\"  F1-Score: {dpn_f1:.4f}\")\n",
    "print(f\"  AUC-ROC: {dpn_auc_roc:.4f}\")\n",
    "print(f\"  AUC-PR: {dpn_auc_pr:.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Classification Report:\")\n",
    "print(classification_report(y_dropout_test, y_dropout_pred, \n",
    "                           target_names=['Not Dropout', 'Dropout'], \n",
    "                           digits=4, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eb1eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and Precision-Recall Curves for DPN-A\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds_roc = roc_curve(y_dropout_test, y_dropout_pred_proba)\n",
    "axes[0].plot(fpr, tpr, linewidth=2, label=f'DPN-A (AUC={dpn_auc_roc:.4f})', color='darkorange')\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "axes[0].set_xlim([0.0, 1.0])\n",
    "axes[0].set_ylim([0.0, 1.05])\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=11)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=11)\n",
    "axes[0].set_title('ROC Curve - Dropout Prediction', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_dropout_test, y_dropout_pred_proba)\n",
    "axes[1].plot(recall, precision, linewidth=2, label=f'DPN-A (AP={dpn_auc_pr:.4f})', color='navy')\n",
    "axes[1].axhline(y=y_dropout_test.mean(), color='k', linestyle='--', \n",
    "                linewidth=1, label=f'Baseline ({y_dropout_test.mean():.2%})')\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('Recall', fontsize=11)\n",
    "axes[1].set_ylabel('Precision', fontsize=11)\n",
    "axes[1].set_title('Precision-Recall Curve - Dropout Prediction', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(loc='best')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# PPN Confusion Matrix\n",
    "cm_ppn = confusion_matrix(y_target_test, y_target_pred)\n",
    "sns.heatmap(cm_ppn, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_labels, yticklabels=target_labels,\n",
    "            ax=axes[0], cbar_kws={'label': 'Count'})\n",
    "axes[0].set_title('PPN Confusion Matrix (3-Class)\\nAccuracy: {:.2%}'.format(ppn_accuracy), \n",
    "                  fontsize=12, fontweight='bold', pad=15)\n",
    "axes[0].set_ylabel('True Label', fontsize=11)\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=11)\n",
    "\n",
    "# DPN-A Confusion Matrix\n",
    "cm_dpn = confusion_matrix(y_dropout_test, y_dropout_pred)\n",
    "sns.heatmap(cm_dpn, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['Not Dropout', 'Dropout'], \n",
    "            yticklabels=['Not Dropout', 'Dropout'],\n",
    "            ax=axes[1], cbar_kws={'label': 'Count'})\n",
    "axes[1].set_title('DPN-A Confusion Matrix (Binary)\\nAccuracy: {:.2%}'.format(dpn_accuracy), \n",
    "                  fontsize=12, fontweight='bold', pad=15)\n",
    "axes[1].set_ylabel('True Label', fontsize=11)\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e08f06",
   "metadata": {},
   "source": [
    "## 5. LLM-Based Personalized Recommendations\n",
    "\n",
    "Using **GPT-4 integration** to generate actionable interventions for at-risk students:\n",
    "- Analyzes student profile (academic, socioeconomic, behavioral)\n",
    "- Identifies specific risk factors\n",
    "- Provides personalized, evidence-based recommendations\n",
    "- Prioritizes interventions by expected impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47203bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze high-risk students\n",
    "high_risk_threshold = 0.7\n",
    "high_risk_indices = np.where(y_dropout_pred_proba > high_risk_threshold)[0]\n",
    "\n",
    "print(f\"ðŸš¨ High-Risk Students Analysis\")\n",
    "print(f\"=\" * 80)\n",
    "print(f\"Total students in test set: {len(y_dropout_test)}\")\n",
    "print(f\"High-risk students (prob > {high_risk_threshold}): {len(high_risk_indices)} ({len(high_risk_indices)/len(y_dropout_test)*100:.1f}%)\")\n",
    "\n",
    "if len(high_risk_indices) > 0:\n",
    "    # Select highest-risk student for detailed analysis\n",
    "    highest_risk_idx = high_risk_indices[np.argmax(y_dropout_pred_proba[high_risk_indices])]\n",
    "    \n",
    "    # Get original data index\n",
    "    original_idx = X_test.index[highest_risk_idx]\n",
    "    student_record = df.loc[original_idx]\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Highest Risk Student Profile:\")\n",
    "    print(f\"-\" * 80)\n",
    "    print(f\"  Student ID: {original_idx}\")\n",
    "    print(f\"  Predicted Dropout Probability: {y_dropout_pred_proba[highest_risk_idx]:.2%}\")\n",
    "    print(f\"  Predicted Outcome (3-class): {target_labels[y_target_pred[highest_risk_idx]]}\")\n",
    "    print(f\"  True Outcome: {target_labels[y_target_test[highest_risk_idx]]}\")\n",
    "    \n",
    "    print(f\"\\n  ðŸ“š Academic Profile:\")\n",
    "    print(f\"    â€¢ 1st Semester Grade: {student_record['Curricular units 1st sem (grade)']:.2f}/20\")\n",
    "    print(f\"    â€¢ 2nd Semester Grade: {student_record['Curricular units 2nd sem (grade)']:.2f}/20\")\n",
    "    print(f\"    â€¢ 1st Sem Approved: {student_record['Curricular units 1st sem (approved)']}/{student_record['Curricular units 1st sem (enrolled)']}\")\n",
    "    print(f\"    â€¢ 2nd Sem Approved: {student_record['Curricular units 2nd sem (approved)']}/{student_record['Curricular units 2nd sem (enrolled)']}\")\n",
    "    \n",
    "    print(f\"\\n  ðŸ‘¤ Demographics:\")\n",
    "    print(f\"    â€¢ Age at Enrollment: {student_record['Age at enrollment']}\")\n",
    "    print(f\"    â€¢ Gender: {'Male' if student_record['Gender'] == 1 else 'Female'}\")\n",
    "    print(f\"    â€¢ Scholarship Holder: {'Yes' if student_record['Scholarship holder'] == 1 else 'No'}\")\n",
    "    print(f\"    â€¢ Debtor: {'Yes' if student_record['Debtor'] == 1 else 'No'}\")\n",
    "    print(f\"    â€¢ Tuition Up to Date: {'Yes' if student_record['Tuition fees up to date'] == 1 else 'No'}\")\n",
    "else:\n",
    "    print(\"\\nâœ“ No high-risk students found in test set with current threshold\")\n",
    "    # Select a medium-risk student instead\n",
    "    medium_risk_indices = np.where((y_dropout_pred_proba > 0.4) & (y_dropout_pred_proba <= 0.7))[0]\n",
    "    if len(medium_risk_indices) > 0:\n",
    "        highest_risk_idx = medium_risk_indices[0]\n",
    "        original_idx = X_test.index[highest_risk_idx]\n",
    "        student_record = df.loc[original_idx]\n",
    "        print(f\"\\nShowing medium-risk student instead (prob: {y_dropout_pred_proba[highest_risk_idx]:.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df2488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate rule-based recommendations (fallback when no LLM API)\n",
    "def generate_recommendations_rule_based(student_data, dropout_prob, predicted_outcome):\n",
    "    \"\"\"Generate recommendations based on decision rules\"\"\"\n",
    "    \n",
    "    recommendations = []\n",
    "    risk_level = \"High\" if dropout_prob > 0.7 else \"Medium\" if dropout_prob > 0.3 else \"Low\"\n",
    "    \n",
    "    # Calculate key metrics\n",
    "    grade_1st = student_data.get('Curricular units 1st sem (grade)', 0)\n",
    "    grade_2nd = student_data.get('Curricular units 2nd sem (grade)', 0)\n",
    "    avg_grade = (grade_1st + grade_2nd) / 2 if (grade_1st > 0 or grade_2nd > 0) else 0\n",
    "    \n",
    "    approved_1st = student_data.get('Curricular units 1st sem (approved)', 0)\n",
    "    enrolled_1st = student_data.get('Curricular units 1st sem (enrolled)', 1)\n",
    "    success_rate = (approved_1st / enrolled_1st * 100) if enrolled_1st > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\" ðŸŽ¯ PERSONALIZED INTERVENTION RECOMMENDATIONS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\nðŸ“Š Risk Assessment: {risk_level} Risk ({dropout_prob:.1%} probability)\")\n",
    "    print(f\"ðŸ“ˆ Predicted Outcome: {predicted_outcome}\")\n",
    "    print(f\"ðŸ“š Average Grade: {avg_grade:.2f}/20\")\n",
    "    print(f\"âœ… Success Rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n{'â”€'*80}\")\n",
    "    print(f\" PRIORITY RECOMMENDATIONS:\")\n",
    "    print(f\"{'â”€'*80}\\n\")\n",
    "    \n",
    "    # Rule 1: Low grades\n",
    "    if avg_grade < 10:\n",
    "        print(\"ðŸ”´ CRITICAL - Academic Performance\")\n",
    "        print(\"   â€¢ IMMEDIATE ACTION: Schedule emergency academic advisor meeting\")\n",
    "        print(\"   â€¢ Enroll in supplemental instruction sessions for struggling courses\")\n",
    "        print(\"   â€¢ Consider reduced course load next semester (max 12-15 credits)\")\n",
    "        print(\"   â€¢ Weekly check-ins with academic success coach\")\n",
    "        print(\"   Expected Impact: HIGH\\n\")\n",
    "    \n",
    "    # Rule 2: Financial issues\n",
    "    if student_data.get('Debtor', 0) == 1 or student_data.get('Tuition fees up to date', 1) == 0:\n",
    "        print(\"ðŸŸ¡ HIGH PRIORITY - Financial Barriers\")\n",
    "        print(\"   â€¢ Connect with financial aid office within 48 hours\")\n",
    "        print(\"   â€¢ Apply for emergency student assistance funds\")\n",
    "        print(\"   â€¢ Explore scholarship opportunities and payment plans\")\n",
    "        print(\"   â€¢ Consider work-study programs for financial support\")\n",
    "        print(\"   Expected Impact: HIGH\\n\")\n",
    "    \n",
    "    # Rule 3: Low success rate\n",
    "    if success_rate < 50:\n",
    "        print(\"ðŸŸ  URGENT - Course Completion\")\n",
    "        print(\"   â€¢ Enroll in study skills workshop\")\n",
    "        print(\"   â€¢ Join peer tutoring program (2-3 sessions/week)\")\n",
    "        print(\"   â€¢ Attend time management and organization seminar\")\n",
    "        print(\"   â€¢ Create structured study schedule with advisor\")\n",
    "        print(\"   Expected Impact: MEDIUM\\n\")\n",
    "    \n",
    "    # Rule 4: No scholarship\n",
    "    if student_data.get('Scholarship holder', 0) == 0 and dropout_prob > 0.5:\n",
    "        print(\"ðŸŸ¢ RECOMMENDED - Financial Support\")\n",
    "        print(\"   â€¢ Research and apply for available scholarships\")\n",
    "        print(\"   â€¢ Meet with scholarship coordinator for eligibility assessment\")\n",
    "        print(\"   â€¢ Complete FAFSA/financial aid applications\")\n",
    "        print(\"   Expected Impact: MEDIUM\\n\")\n",
    "    \n",
    "    # Rule 5: Age factor\n",
    "    age = student_data.get('Age at enrollment', 20)\n",
    "    if age > 25:\n",
    "        print(\"ðŸ”µ SUPPORT - Non-Traditional Student Services\")\n",
    "        print(\"   â€¢ Connect with adult learner support services\")\n",
    "        print(\"   â€¢ Explore flexible scheduling options\")\n",
    "        print(\"   â€¢ Join non-traditional student peer group\")\n",
    "        print(\"   Expected Impact: MEDIUM\\n\")\n",
    "    \n",
    "    # General recommendations\n",
    "    print(\"ðŸ“Œ GENERAL SUPPORT STRATEGIES:\")\n",
    "    print(\"   â€¢ Utilize campus tutoring center (free services)\")\n",
    "    print(\"   â€¢ Attend professor office hours regularly\")\n",
    "    print(\"   â€¢ Join study groups for challenging courses\")\n",
    "    print(\"   â€¢ Access mental health and wellness resources\")\n",
    "    print(\"   â€¢ Participate in academic success workshops\\n\")\n",
    "    \n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return \"Recommendations generated successfully\"\n",
    "\n",
    "# Generate recommendations for the selected student\n",
    "recommendations = generate_recommendations_rule_based(\n",
    "    student_record.to_dict(),\n",
    "    y_dropout_pred_proba[highest_risk_idx],\n",
    "    target_labels[y_target_pred[highest_risk_idx]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cc546e",
   "metadata": {},
   "source": [
    "## 6. Summary and Next Steps\n",
    "\n",
    "### âœ… Completed in This Notebook:\n",
    "\n",
    "1. **Data Exploration** - Analyzed 4,424 real students with 35 features\n",
    "   - Target distribution: Graduate (49.9%), Dropout (32.1%), Enrolled (18.0%)\n",
    "   - Comprehensive visualizations of demographics, academics, socioeconomics\n",
    "\n",
    "2. **Data Preprocessing** - Journal methodology implementation\n",
    "   - Z-score standardization for numerical features\n",
    "   - Stratified 70-15-15 train-validation-test split\n",
    "   - Class balancing with computed weights\n",
    "\n",
    "3. **Deep Learning Models** - State-of-the-art architectures\n",
    "   - **PPN**: Performance Prediction Network (3-class classification)\n",
    "   - **DPN-A**: Dropout Prediction with Self-Attention (binary + interpretability)\n",
    "\n",
    "4. **Comprehensive Evaluation** - Multiple metrics\n",
    "   - Classification reports with precision, recall, F1-scores\n",
    "   - Confusion matrices for error analysis\n",
    "   - ROC and Precision-Recall curves\n",
    "   - AUC-ROC and AUC-PR scores\n",
    "\n",
    "5. **Personalized Recommendations** - Rule-based intervention system\n",
    "   - Risk stratification (Low/Medium/High)\n",
    "   - Prioritized action items\n",
    "   - Expected impact assessment\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š Key Results Summary:\n",
    "\n",
    "| Model | Task | Accuracy | F1-Score | AUC-ROC |\n",
    "|-------|------|----------|----------|---------|\n",
    "| PPN | 3-class outcome prediction | See above | Macro F1 | N/A |\n",
    "| DPN-A | Binary dropout prediction | See above | Binary F1 | See above |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš€ Next Steps for Publication-Ready Work:\n",
    "\n",
    "1. **Baseline Comparisons** (Required for journals)\n",
    "   - Implement Random Forest, XGBoost, SVM, Logistic Regression\n",
    "   - Statistical significance testing (McNemar's, Friedman test)\n",
    "\n",
    "2. **Advanced Feature Engineering**\n",
    "   - Implement all 12 derived features from methodology\n",
    "   - SHAP analysis for feature importance\n",
    "   - Permutation importance testing\n",
    "\n",
    "3. **Hybrid Multi-Task Model** (HMTL)\n",
    "   - Implement shared trunk with dual prediction heads\n",
    "   - Joint optimization with weighted loss\n",
    "\n",
    "4. **Cross-Validation**\n",
    "   - 10-fold stratified CV for robust evaluation\n",
    "   - Repeated K-fold (5 repetitions)\n",
    "   - Report mean Â± std across folds\n",
    "\n",
    "5. **LLM Integration**\n",
    "   - Integrate OpenAI GPT-4 API for advanced recommendations\n",
    "   - Prompt engineering optimization\n",
    "   - Recommendation quality evaluation\n",
    "\n",
    "6. **Visualization Suite**\n",
    "   - Learning curves, calibration plots\n",
    "   - Feature importance visualizations\n",
    "   - SHAP waterfall and beeswarm plots\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“š Documentation:\n",
    "- **Full Methodology**: `docs/JOURNAL_METHODOLOGY.md`\n",
    "- **Project Overview**: `README.md`\n",
    "- **Quick Start Guide**: `QUICKSTART.md`\n",
    "\n",
    "### ðŸ’» Run Full Pipeline:\n",
    "```bash\n",
    "python main.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Dataset**: 4,424 students | **Features**: 35 | **Models**: 2 deep learning architectures  \n",
    "**Publication Target**: IEEE Transactions on Learning Technologies, Computers & Education"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
